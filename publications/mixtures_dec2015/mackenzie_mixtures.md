## Overview

- the case study – a social media platform that hosts ~20 million code repositories; 
- code and field of devices: 'co-device analysis'
- diagrammatic cultures and data:  the political economies of counting and graphing
- obstacles to and potentials for count-based ethnographic sensibility


## case study in 2 senses:

- a study of coding as a practice, and the way that code links into government, media, science, business and civil society
- a study of working with data amidst the shifting methodological conditions of big data, and the like.


## the case study – a social media platform that hosts ~20 million code repositories; 

I wanted to talk about a year long project I did during 2013-2014, funded by the ESRC and also sponsored by Google Corporation. Along with 4-5 other funded projects, my project called 'metacommunities of code' sought to analyse how code was shared being shared across millions of software repositories.

Some context here for code sharing:

1. We are used to hearing about the importance of code and programming. Here in the UK, the government mandated last year that all children will learn to code because coding is a vital economic activity. 
2. Coding practices have changed over the last decade or so in fairly profound ways, which I won't detail here. The principal changes are a broad cultural shift in the organization of software development. It has become a much more public activity, and the publicness of code has become a major concern for business, government and science, for different reasons.

The social media site Github, started in in 2007, epitomises these changes:
1. it has grown tremendously in the last 7 years (like many social media platforms) to include around 29 million software projects. When I started the project, that number as around 13 million, but even in the last year it has grown tremendously. 
2. this growth flows from a tremendous variety of processes that are difficult to summarise or classify partly because the topics or domains of coding are so diverse and partly because much of what goes on in Github is quite social but also technical. Whether we summarise or classify them or not matters less than the sheer fact of their existence.  

How does Github describe itself. In June 2014, Github showed a photo of a women working in an urban office location, with lights and professional camera focused on here work. It described '6.1 million people collaborating right now across 13.2 million repositories ... building amazing things together'. In November 2015, Github has a slightly more functional description:

>GitHub is how people build software. With a community of more than 11 million people, developers can discover, use, and contribute to over 29 million projects using a powerful collaborative development workflow. [Github/About](https://github.com/about)

## diagrammatic cultures and data:  the political economies of counting and graphing

The other aspect of this case study concerns Google and the ESRC. Both of these bodies have a certain investment in data and digital technologies. While Google's interest in that will be well-known, the ESRC's might be less familiar. Here's the call for applications:

>The ESRC and Google are pleased to announce the Google Data Analytics Social Science Research Call. Data is the new raw material of the 21st century. It allows citizens to hold governments to account, drives improvements in public services by informing choice, and provides a feedstock for innovation and growth. As open-source data and data integration grows, it is a key time to better understand how it maps onto and possibly significantly strengthens, the ability of academics to understand society. 

>The ESRC spends between £15-20 million per year on the collection and curation of data, providing access to a wide variety of data resources and developing the methodological tools and techniques for building and exploiting data resources. This has helped to create a world-leading data infrastructure. Much of this funding has been focused on the creation and support of large scale surveys and other aggregate statistics provided by other major agencies like the ONS. However, as the volume of less structured online data resources become increasingly available the ESRC has recognised the potential of capturing and connecting huge data flows in new ways to support conceptual, methodological and empirical advances in social and economic research. 

> As a first step we are collaborating with Google to identify demand from the social science community in working with online data and to develop a suite of demonstrator projects which highlight its true value for better understanding society.


Much in this call for proposals will be familiar: 'data is the new raw material of the 21st century' points to newness, nowness, and economic potential of work. The following sentences invoke citizens, choice, public services, innovation, growth, and, last but not least, 'academics' understanding society. It then contrasts existing investments in data collection and curation, particularly surveys and statistical data, but contrasts this data with the 'new data,' the 'huge data flows.' Finally, 'as a first step,' the Google collaboration aims to identify 'demand,' as if social science researchers working with data are a kind of consumer, but a consumer who produces 'understanding.' 

## Working with large numbers

To all of this, we might simply respond with a well-known STS standpoint on numbers, and particularly the political economies of large numbers. From the work of Helen Verran, for instance, we can see that this work on 'raw materials' is bound to involve political differences, and that these differences will affect any understandings that depend on that data.  

>In any practical going-on with numbers, what matters is that they can be made  to work, and making them work is  a politics. Yet is a politics that completely evades conventional foundationist analysis (Verran, 2001: 88)

The data raw material imaginary nearly always includes large numbers. As we have already seen with Github, numbers such as 29 million or 11 million come up straight away. Given that the project was funded by the ESRC, how could we go on with large numbers? What kind of going-on with numbers would I,  as an STS researcher, engage in?  

The project was awarded to work on Github using data that the platform makes available through its Application Programmer Interfaces or APIs. In nearly all social media or web2.0 settings, this data embodies the traces of social practices of many different kinds criss-crossing digital devices today. The API data was not created as a data resource for social scientists, but for software developers working in convergence cultures [@Jenkins_2004] in which connecting different platforms, devices, sites and practices together using code has become standard practice. 

This paper concerns the work needed to get contemporary digital data and particularly, large numbers numbers, to do something other than confirm what we already know. This work is somewhat ethnographic in its ambitions. That is, it hopes to be a form of writing that derives from an encounter with some particular differences or alterity that challenges accepted understandings of what is happening. The ethnographic setting here is not a field site, but principally a stream of timestamped event data from a social media platform. Rather than travelling some place to do ethnography, I'm asking: how can can we treat a body of data ethnographically as a kind of field site? That means finding a place to sit in relation to the data and learning some ways of having a conversation with it, a conversation that will include questions about how many and how often, but will also include the experience of being located in the data. The question here is: how might deliberate and somewhat naïve preoccupation with digital data become not

