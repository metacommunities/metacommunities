# On recursive analytic affiliation: reflecting on the altered conditions of ethnographies of coding

## Introduction

I report here on a deliberately naive attempt to re-count a single number: approximately 29 million code repositories on [Github](http://github.com)  at a particular point in time (late 2015). The number appeared in a research  project primarily focused largely on transformations in the social life of coding, programming and software development.  The project  sought to experiment with data-intensive methods, infrastructures and tools -- cloud computing services such as Google Compute, data analytic and visualization tools such as `R,` `Python` and `IPython` notebooks, large data stores such as` BigQuery`, streaming data from social media platform APIs (Application Programmer Interfaces), predictive models especially in the form of machine learning classifiers (support vector machines, Naive Bayes classifiers, random forests) --  in making sense of what happens on  Github *en masse.[^12]*  Somewhat recursively (although I don't want to make too much of this recursion, since I do not think it amounts to a recursive public [@Kelty_2008]), Github happens to be the media platform that hosts the development of many of the big data tools and infrastructures (`hadoop`, `tensorflow`, `ipython`, `flow`, `d3.js`,  `spark`, etc.), and it also directly hosts  all of the text, code, figures, intermediate results and configuration information for the research project itself, including this chapter in various operational and executable forms [@Metacommunities_2016].

The approximate number 29 million  will in this chapter serve as a synecdoche for big data in several ways. The number, a count of all the repositories on Github in late 2014, is a part standing in for the whole. On the one hand, Github as a platform used by millions of people is precisely the kind of setting open to, affected and shaped by big data practices. On the other hand, what happens on Github -- coding, software development, configuration, modification and transformation of information infrastructures -- is very much at the heart of  big data practices. Focusing on just one biggish number --  29 million -- that presents itself as sum of all repositories is inevitably severely myopic. 

The story of an encounter with this single number will, I hope, evoke and exemplify the appeal, frustrations, hopes and some material configurations associated with contemporary data practices that envision working with 'all the data' [@Mayer-Schonberger_2013]. This number, one of a series of similar numbers,  exists and exerts effects in different ways on counting and accounting. I'm interested in its composition, its instability, the 'stories,' imaginings and materialisations it authorises, and its practically problematic relation to big data infrastructures.

What kinds of engagement can we have with such numbers today amidst perhaps their deep transformation and re-materialisation in technologies? Given so much computational counting, so many efforts to count all manner of things (species, clicks,  stars, likes, cells and bitcoin transactions), and indeed strong injunctions to tell stories about such numbers, what can social researchers do? The key difficulty in developing an ethnographic sensibility around such numbers resides in the many detours, variations and differences they embody, not least for the social researcher who finds him or herself trying to re-count them or account for them. The  chapter documents  and practically re-enacts a  series of queries  --  both in the technical sense of a statement executed by a database and a question posed to a given state of affairs.

While I'm very drawn to attempts to count things and to weave counts into ethnographic accounts, during this project I often wondered about how to count things whilst paying attention to their composition. Numbers and counts are site of entanglement between device-specific materialities and imaginaries of order, totality, rank, inclusion, boundary and power.  This double sense of numbers as both a calculatively ordered cutting-framing-summing-up and as materially specific figuring of differences could well be understood in terms of what Lucy Suchman's describes as a 'configuration': 

>the device of _configuration_ has two broad uses. First, as an aid to delineating the composition and bounds of an object of analysis, in part through the acknowledgment that doing so so is integral not only to the study of technologies, but to their very existence as objects. And second, in drawing our analytic attention to the ways in which technologies materialize cultural imaginaries, just as imaginaries narrate the significance of technological artefacts. Configuration in this sense is a device for studying technologies with particular attention to the imaginaries and materialities that they _join together_, an orientation that resonates as well with the term's common usages to refer to the conjoining of diverse elements in practices of systems design and engineering [@Suchman_2012, 48].

While Suchman's examples of configuration come from software systems, I'm suggesting that a single number -- 29 million repositories on Github -- could be re-counted as a configuration. The argument is that even the listing, enumerating and counting of things in a single number can be seen as a configuration. 

## Re-counting a capital number on  a platform

On Github, two numbers have cardinal and indeed capital importance:  the number of people using the platform, and the number of different code repositories stored there.  I deal in this chapter only with the latter.   The  counting I undertake will focus on problems of copying, duplicating, and imitation that render large numbers a moving substrate.

Github, started in in 2007, epitomises and has indeed been central -- as the suffix 'hub' suggests -- to a mass of configurational events associated with the development of big data practices. Like many social media platforms,  it has grown tremendously in the last 7 years  to include around 38 million software projects (September 2016). This growth flows from a tremendous variety of processes that are difficult to summarise or classify partly because the actors, topics or domains of coding are so diverse and partly because much of what flows through Github is both technically and socially 'innovative' in the sense that Bruno Latour uses the term: '" innovative" means that we do not know the number of actors involved in advance' [@Latour_1996, 72]. 

Amidst this morass of code, configuration work, copying, collaborating, and much playing around,  Like many contemporary assemblages, Github seeks to describe itself through large numbers. We might call these  'capital numbers' since they serve a vital function in attracting investment, imbuing the platform with hub-like importance in contemporary innovation systems, and actually, as I will soon suggest, directly support the injunction to 'tell stories with the [big] data.'  These numbers in their sometimes daily changes and updates attest to an investment in being innovative or open-ended enumerations.  

As Suchman's concept of a configuration suggests, capital numbers combine with cultural imaginaries in manifold ways.   In June 2014, the Github 'about' page showed a photo of a women working in an urban office location, with lights and professional camera focused on her work. It described '6.1 million people collaborating right now across 13.2 million repositories ... building amazing things together' [@Github_2014a]. In late November 2015, the same page has a slightly more functional description, and the image seems to be of a press conference in China:

>GitHub is how people build software. With a community of more than 12 million people, developers can discover, use, and contribute to over 29 million projects using a powerful collaborative development workflow. [@Github_2015](https://web.archive.org/web/20151216055610/https://github.com/about)

The numbers change over time alongside the composition of the actors involved (women coding and working at Github, a sore point for the company as a co-founder was involved in sexual harassment complaints; Chinese developers using Github, but also political activists, leading to a denial of service attack on the Github, allegedly by the Chinese government [^3]). But the changing numbers, alongside the sometimes rather sublime scales of community (a 'community' of 12 million people?) certainly form part of Github's description.


## The litany of sense-making: enumerating the aggregate through events

In 2013, a time in which the injunction to do things with data was impacting social science research funders as well as many people in media, commerce, industry and government,  my data-analytic ambition was that I could critically  re-count the capital numbers -- people and repositories on Github -- in some way. I planned to use data analytic methods to count people and count things, in this case, software projects on Github. In re-counting them, I wanted to en-counter something of their composition in all its heterogeneity and conformity, to show how their scale was an effect of specific dynamics of imitation, and perhaps to suggest that such numbers were highly performative in essence.  This would be a both political and culturally significant since those numbers are typical of social media platforms and their capitalisation. 

How could I  actually count software project or people? Like everyone else, I turned to data published through the Github API (Application Programmer Interface). APIs  were not created as a data resource for social scientists, but for software developers working in convergence cultures [@Jenkins_2004] in which connecting different platforms, devices, sites and practices together using code has become standard practice, APIs have great practical significance [@Bucher_2013]. In Github's case, data published through the API derives from acts of writing and reading code (although not only code -- an extraordinary variety of other documents, texts, images, maps, and other forms of data can be found on the platform).   The data is logged  as developers write code and move  that code in and out of code repositories using the `git` version control system [@Git_2014]. Coding today constructs and take place in increasingly complex associative infrastructures.  Github seeks to figure coding as a social network practice based on a hub.

By convention much API data from social media sites has an 'event' structure that links named actors and named entities to specific infrastructural locations (usually coded as an URL) at a particular time (the 'timestamp'). While not all big data has this timestamp-actor-location-links texture, it is common enough to stand in as typical of the data that big data infrastructures and practices encounter.  

```
{
"id": "2111998059",
"type": "WatchEvent",
"actor": {
"id": 1459103,
"login": "mmemetea",
"gravatar_id": "4532d1e4885f579ca7d9aa8748418817",
"url": "https://api.github.com/users/mmemetea",
"avatar_url": "https://avatars.githubusercontent.com/u/1459103?"
},
"repo": {
"id": 14802742,
"name": "OpenSensorsIO/azondi",
"url": "https://api.github.com/repos/OpenSensorsIO/azondi"
},
"payload": {
"action": "started"
},
"public": true,
}
```

What would it mean to count things using this data? These records conjoin elements together.  The relatively simple  `WatchEvent` on Github shown in the data extract above documents how an `actor` calling themselves `mmemetea` associates as a `Watcher` with a  repository called `azondi`, a software project coordinated by the 'organisation' `OpenSensorsIO`.[^2]  Note that the event also has various attributes -- it is a `public` event, it has a 'payload' (often much more complicated than simply `started`) -- and includes various indexical references or `id`s that link the event to other groups of people, organisations, repositories and images (`gravatar_id`). The intricate  syntax of this data -- many brackets, inverted commas, colons, commas -- attests to a complex social configuration which links actors, actions, places and times in discrete events ordered in time. The discreteness of components in this event - actor, organisation, repository, payload, gravatar, etc. --  attests to the potential for the precise configuration of elements around a given repository to change or rearranged in some way. New actors might be added; relations might appear between entities; the location of entities might shift, and forms of association ('organizations') might subsume or grow out or around all of this. In short, each event in the timeline API suggests another small re-configuration in the totality of elements comprising Github. 

## Imbuing numbers with importance

When our project started in 2012, we were clearly not the only people interested in counting things amidst the massive stream of event data concerning Github. A dataset purporting to contain the whole public event timeline of Github suddenly appeared in mid-2012. Ilya Grigorik, a 'Web Performance Engineer' at Google, launched a Github repository `igrigorik/githubarchive` linked to a website `GithubArchive.org` dedicated to archiving all the Github API public event data -- the so-called 'timeline' --  in one place [@Grigorik_2012].  Grigorik, or `igrigorik`not only published all the data in a cloud-based data store but published that data  in Google's newly launched cloud computing service, GoogleBigQuery.[^1] The [Github timeline data](https://bigquery.cloud.google.com/table/githubarchive:github.timeline) was listed, along with all the words in Shakespeare and all the US birth name records, as one of three dataset exemplars that people could use to learn about Google `BigQuery` [@Google_2016]. Like the data on GithubArchive itself, the Google `BigQuery` copy of the Github public timeline data was updated hourly. Reaching back to  2011, it logs around 400 million public events.

The archiving, copying and transformation of Github event data into  big data-ready format drew the attention many people. The data was publicised through several 'Github Data Challenges' (2012-2014). Working to count events in different ways using data analytic techniques and data visualizations, people re-counted events on the Github timeline to in order to tell different 'stories' about Github's millions of repositories and people.   These stories include many different configurations. For instance,  the 'OpenSource Report Card' (http://osrc.dfm.io/)  or `dfm/osrc` by Dan Foreman-Mackay [@Foreman-Mackay_2014], is a prize-winning use of the timeline data (see Figure \ref{fig:osrc}). It ingests all the data from the Githubarchive, counting what developers do, when they do it, and using what programming languages. With this data stored, it then builds a predictive model that allows it to both profile a given Github user in terms of the mixture of programming language they use, and to predict who that Github user might be similar to. Here the mass of events in the Github timeline are brought to bear on finding similarities between people, producing numbers and score to suggest similiarities in coding work. 

In response to the Github Data Challenge in 2012, people looked for feelings or 'sentiments'  about different programming languages in the timeline data.  Feelings associated with coding were mined by counting emotional words present in comments accompanying the Github events (http://geeksta.net/geeklog/exploring-expressions-emotions-github-commit-messages/). The presence of words in these message can be cross-linked with programming languages in order to profile how different programming languages elicit different emotional reactions. 

Capital numbers were also *nationalised* or regionalised. People  mapped coders and repositories by geographic location. The  mapping of Github contributions by location performed by David Fischer (http://davidfischer.github.io/gdc2/#languages/All) is typical in that it too counts events, but this time puts the emphasis on the geography of the 'top' repositories, coders and their programming languages. As the David Fischer puts it 'this data set contains contributions to the top 200 GitHub repositories during the first four months of 2013 and plots the location based on what the contributor provided' [@Fischer_2013a].  

*Logistic numbers* can be derived from the data streams. People made live dashboards, a characteristic data analytic visual form, for Github. `Octoboard` (http://octoboard.com/) animates changes on Github using the timeline data [@Roussell_2015] (see Figure \ref{fig:octoboard}). It ornaments the capital numbers with a range of peripheral live enumerations that point to the liveliness of events on Github. It presents a summary of daily activity in major categories on Github – how many new repositories, how many issues, how repositories have been 'open sourced' today. It offers realtime analytics on emotions. Like many other dashboards associated with social media analytics, `octoboard` suggests that the constant change in associations and projects in software development can no longer be known through leisurely rhythms of analysis, but is increasingly framed as  a problem  realtime awareness. 

Looking slightly more widely, the Github timeline data has quickly become a favourite training tool for data mining textbooks books that configure and convey the calculative agencies characteristic of capital numbers. In  *Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More,* Matthew Russell makes use of the Github timeline to demonstrate ways of uses social network analysis to highlight the important nodes and links between repositories and users [@Russell_2013]. Finally and not least for my own projects, academic researchers in computer science and certain parts of management science, GithubArchive and the `BigQuery` publication of Github  has been a boon because it permits relatively easy  study technologically and economically important practices of software development.  Academic researchers in fields such as software engineering do social network analysis in order to gauge productivity, reuse, efficiency and other engineering and management concern [@Thung_2013]. Like the many Github-hosted projects discussed above, they analyse sentiment [@Guzman_2014], collaboration and productivity [@Dabbish_2012], and geography [@Takhteyev_2010].

All of these re-countings enliven, animate, reactivate, localise and qualify the capital numbers. They potentialise the numbers in terms of work, geography, liveness  and further accumulation by summing them up in different ways (realtime status updates and dashboards, networks of associations, geographies of work and affect) commonly found in contemporary data economies and as the outcome of big data practice.  Many of the dashboards, maps, sentiment analyses and predictive recommendations are very directly part of big data practice, and the fact that people using Github should so readily analyse Github itself using big data infrastructures such GoogleBigQuery and other analytic devices is hardly surprising. Coders and software developers are, after all, key workers in the ongoing transformation of systems of controls and configuration associated with  big data. 

## Acts of imagined accumulation

Given this vortex of work around the Github data, which I interpret as both symptomatic of  big data practices and as a set of parallel 'storyfications' of the 'datafication' of Github, where would an ethnographic sensibility intervene? Is there any place or point at which it might engage with the code-infrastructure oriented practices associated with flows of data? Given my -- and indeed 'our', since this project was a team effort, including statisticians and social theorists --  propensity or somewhat unwitting interpellation as a white, middle-class, middle-age, relatively technically over-skilled male social researcher long-drawn into information infrastructural transformations, the chances of an ethnographic attentiveness to differences, slippages, experiential ambivalences and ambiguities actually having an chance of purchase were rather slim. Could writing queries for GoogleBigQuery GithubArchive dataset, developing data analytic and visualizing code in `R` and `python`, programming languages commonly used in the  big data practices actually generate any auto-ethnographic resonance? 

The basic capital number of the repository count was usefully grounding. Technically, the number is relatively easy to count in the GithubArchive data. A simply GoogleBigQuery produces the number in roughly 2.0 secs having processed around 40Gb of URLs:

```{r repo_count, cache=TRUE, echo=TRUE, warnings=FALSE}
library(bigrquery)
query1 = "SELECT count(distinct(repo.url))
    FROM 
    [githubarchive:year.2011],
    [githubarchive:year.2012],
    [githubarchive:year.2013],
    [githubarchive:year.2014],
    [githubarchive:year.2015]"
    repo_count = query_exec('metacommunities', query = query1)
```

The result: `r repo_count`

How could we re-count such numbers capital numbers in order to both further highlight their dynamic composition through flows of association and to highlight some of the highly reactive, imaginary boundary work they might do as the materialize imaginaries of totality, globality or infrastructural control? Could configurative numbers in both their compositional and imaginary-materialising multiplicity be counted? Whilst there is a much configurational work involved even in beginning to count things heterogeneously on an infrastructural scale, I found myself drawn in two directions, one concerned with transience of configurations and the other with their imitative composition. Both concerns -- transience and imitation -- are not highlighted in any of the other re-counts of the GithubArchive data, but they only became salient to me amidst many other failures to find any interesting story, stable signal or statistical regularity in the Github timeline data.[^40]

[^40]: Nearly all of our attempts to make sense of the data can be found in the Github  repository [https://github.com/metacommunities/metacommunities](https://github.com/metacommunities/metacommunities). The scripts, plots and pieces of writing found there, however, are not highly ordered. The key results lie scattered across different branches of that repository. The somewhat tangled, messy aggregate of materials there attests to our strenuous but often incoherent efforts to find valid statistical regularities in a dataset characterised by tremendous heterogeneity and messiness. 


When we count people or things (such as software project), the working assumption is that they have some duration and substantial presence. This assumption, however,  does not hold very true at the scale of large data-streams, where, like sub-atomic particles in a collision, people and things flash in and out of existence quite rapidly and unpredictability. Frustrated by our seeming inability to find anything in the GithubArchive data apart from the obvious importance of well-known software projects such as `linux` or `android`, we were forced to accept transience and ephemeral visibility as a common mode of existence on Github. The transient visibility of people and things in the data takes many forms.  The vast majority of repositories on Github are very short-lived. They attract one or two events. A great proportion of events in the timeline data were absorbed into ephemeral repositories (if that is not too great a contradiction in terms).  Millions of repositories flash into visibility on the timeline for a brief period before falling back into obscurity. Yet they remain in the capital number. The diagram shown in the Figure \ref{fig:power_time} encapsulates this imitative flux. On the left hand side, millions of repositories receive less than five events during the 18 months. On the right hand side, less than 50 repositories receive more than a thousand events. A similar pattern appears in the other capital number: while some 'people'  emit many thousands of events, others only trigger a few.[^5] Already, then, the capital number of repositories on  Github takes on a different composition when viewed from the perspective of duration. In the number '29 million,' 22

The massive asymmetry between the relatively few long duration repositories and the vast mass of transient, abandoned repositories is definitely not another corroboration of 'the long-tail' distributions that social media exponents such as Clay Shirky and Chris Anderson began discussing more than a decade ago and that may have had a large part to play in many of the intensely individualizing tendencies of big data analytics, targetted advertising, predictive feeds and recommendations. (For instance, one reason that Amazon.com stocks so many obscure products, is that the long tail of sales of these items is  potentially more important than the sales of a smaller number of best-sellers [@Byrnnjolfsson_2006; @Anderson_2009].)   Rather than the long tail of coding, or a scale-free network of code-hubs on Github (itself a hub in networks of code-related infrastructure), or even simply waste, noise or something to be discarded, we might trace the working of associative processes through highly skewed distribution of events. I can only give a brief indication of this here. For instance, counting just the unique names of repositories in the timeline data, counting how often and when they appear as event in the data stream begins to suggest something about the composition of the capital number of repositories. Almost two thirds of the repositories in Github inject only one or two events into the timeline data stream ever. This means that 20 million of the 30 million repositories are just flashes in the timeline data appearing as one or two events. 

```{r fork_count, echo=FALSE, cache=TRUE, warnings=FALSE}
library(bigrquery)
query = "SELECT count(*) cnt
FROM [githubarchive:year.2011], [githubarchive:year.2012],[githubarchive:year.2013],[githubarchive:year.2014], [githubarchive:year.2015]
where type=='ForkEvent'"
df = query_exec('metacommunities', query=query)
f = round(df[1], 2)
```


```{r event_count, echo=FALSE, cache=TRUE, warnings=FALSE} 
library(ggplot2)
library(bigrquery)
library(dplyr)
query2 = "SELECT type, count(type) cnt FROM
    [githubarchive:year.2011],
    [githubarchive:year.2012],
    [githubarchive:year.2013],
    [githubarchive:year.2014],
    [githubarchive:year.2015]
    GROUP BY type
    ORDER BY cnt DESC"
df = query_exec('metacommunities', query=query2)
g = ggplot(df, aes(x=type, y=cnt)) + geom_bar(stat='identity') 
g + coord_flip()
social_events = df %>% filter(type %in%  c('WatchEvent', 'IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent', 'ForkEvent')) %>% summarise(sum(cnt))
proportion = round(social_events/sum(df$cnt)*100, 0)
```

What are these events? The event of creating a named repository is primary, followed by the event of pushing (putting something into a repository -- a 'commit'), and then  act of copying ('forking') another repository. People 'fork' other repositories frequently. If we just count the event of creating a repository by copying or forking it, more than half all repositories are copies of existing repositories (`r f `). If every ForkEvent on Github creates a new repository, more than half of the repositories are direct copies. Acts of copying occur on many scales and at various levels of infrastructural and associative complexity. This copying is vital to the 'sharing' practice of Github coding. Of the several hundred million events in the timeline dataset, approximately `r social_events` or `r proportion`% of all events in the Github timeline data arise from copying, watching, commenting on or otherwise observing other repositories.(See the WatchEvent, ForkEvent, PullRequestEvent,and IssueEvent counts in figure \ref{fig:event_count}.)    

## The growth of associative imitation

```{r android_associations, echo=FALSE, cache=TRUE}
    library(bigrquery)
    library(ggplot2)
    query_android = "SELECT repo.name, COUNT(type) AS cnt, DATE(created_at) AS date, YEAR(created_at) AS yr, MONTH(created_at) mth
        FROM
        [githubarchive:year.2011], [githubarchive:year.2012], [githubarchive:year.2013], [githubarchive:year.2014], [githubarchive:year.2015],
        TABLE_DATE_RANGE([githubarchive:day.], TIMESTAMP('2016-01-01'), TIMESTAMP('2016-06-31'))
        WHERE
        type= 'ForkEvent' AND repo.name CONTAINS('android')
        GROUP BY
        repo.name, date, yr, mth
        HAVING
        cnt > 3
        ORDER BY
            yr, mth, cnt DESC"
    df_and = query_exec('metacommunities', query= query_android)
    head(df_and)
```

While it is hard to grasp the texture of imitative processes in event data, they clearly figure deeply in the composition of the capital numbers. We can observe some  localised aspects  of propagation of imitation. In the years 2012-2014, both mobile devices and user interfaces were being intensively transformed in coding work in complex ways (e.g. the growth of apps, and the transformation of webpages from static HTML-formatted text to thoroughly dynamic script-driven mutables).   Both Figure \ref{fig:bootstrap_forks} and \ref{fig:android_forks} count imitations in the form of copying code, but in ways that go beyond the formal copying mechanisms offered by Github itself (for instance in the 'Fork' button that appears on the top right hand side of any repository on Github; for example [https://github.com/twbs/bootstrap](https://github.com/twbs/bootstrap)). Imitations appears in two different ways in these figures. The broad bands of color rippling horizontally  across the middle of the figures graph the counts of copies being made each day on Github of popular repositories using the Fork action. (In forking, the repository name remains the same.)   But the much more dense striations  on either side of the central bands, seen for instance in Figure \ref{fig:android_forks}, count repositories whose names incorporate the important repository, but vary it in some way. These repositories have some association with the `android` repository, but diverge from it in a multiplicity of ways. A repository may, for instance, relate to the popular  Twitter `bootstrap` repository yet combine it with a range of other platforms and devices such as `android` or `jQuery`. 

In striated zones of associative imitation, it is possible to count something different. Imitations augment the average everydayness of coding work with forms of associative investment and affective identification.  In this work, important repositories act as high visibility markers around which forms of identity take hold and multiply. Unlike the capital numbers with their total aggregates of people or repositories, the imitative fluxes that bulk out and pad in these diagrams have diverse networks of association.  The implication here is that by counting elements within the capital numbers, we can begin to glimpse the pathways of generative circulation that enliven and literally join together in contemporary technologies.     

## De-capitalising configuration

