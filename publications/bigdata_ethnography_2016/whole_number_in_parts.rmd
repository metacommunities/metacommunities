--- 
title:  "On recursive analytic affiliation: reflecting on the altered conditions of ethnographies of coding"
author: Adrian Mackenzie 
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 12pt
citation_package: biber
latex_engine: xelatex
bibliography: /home/mackenza/ref_bibs/uni.bib
---

## Introduction

I report here on a deliberately naive attempt to re-count a single number: approximately 29 million code repositories on [Github](http://github.com)  at a particular point in time (late 2015). The number appeared in a research  project primarily focused largely on transformations in the social life of coding, programming and software development amidst apps, clouds, virtualization and the troubled life of code commons.  In exploring 'how people build software' [@Github_2015], the project  explicitly sought to experiment with data-intensive methods, infrastructures and tools -- cloud computing services such as Google Compute, data analytic and visualization tools such as `R,` `Python` and `IPython` notebooks, large data stores such as` BigQuery`, streaming data from social media platform APIs (Application Programmer Interfaces), predictive models especially in the form of machine learning classifiers (support vector machines, Naive Bayes classifiers, random forests) --  in making sense of what happens on  Github _en masse._  Somewhat recursively (although I don't want to make too much of this recursion, since I do not think it amounts to a recursive public [@Kelty_2008]), Github happens to be the *platform*  that hosts the code development of many big data tools and infrastructures (`hadoop`, `tensorflow`, `ipython`, `flow`, `d3.js`,  `spark`, etc.). It also directly hosts  all of the text, code, figures, intermediate results and configuration information for the research project I describe, including this chapter in various operational and executable forms [@Metacommunities_2016].

The approximate number 29 million  will in this chapter serve as a synecdoche for big data in several ways. The number, a count of all the repositories on Github in late 2015, is a small part standing in for the whole. On the one hand, Github as a platform used by millions of people is a typical setting open to, affected and shaped by big data practices. On the other hand, what happens on Github -- coding, software development, configuration, modification and transformation of information infrastructures -- shapes  big data practices. Crucially, open or publicly visible code repositories are shadowed by largely invisible private repositories that yield revenue for Github as a San Francisco-based business, it is inherently difficult to count all the repositories. 'All' here inevitably means 'some,' but maybe that is always a problem for big data.      

A focus limited to one big number  (29 million), particularly one that presents itself as the total of all repositories, is inevitably narrow. The story of encounters with this single number might, however, evoke and exemplify the appeal, frustrations, hopes and  material configurations associated with contemporary data practices that envision working with 'all the data' [@Mayer-Schonberger_2013]. This number, one of a series of similar numbers,  exists and exerts effects in different ways on counting and accounting. I'm interested in its composition, its instability, the 'stories,' imaginings and materialisations it authorises, and its practically problematic relation to big data infrastructures.

What kinds of engagement can we have with such numbers today amidst perhaps their deep transformation and re-materialisation in technologies?[^16] Given so much computational counting, so many efforts to count all manner of things (species, clicks,  stars, likes, cells and bitcoin transactions), and indeed strong injunctions to tell stories about such numbers, what can social researchers do? One key difficulty in developing an ethnographic sensibility around such numbers resides in the many detours, variations and differences they embody, not least for the social researcher who finds him or herself trying to re-count them or account for them.  These variations, shadings and shifting in numbers might be seen as the everyday frictions of ethnographic writing in its reflexive analytic mode, but they also spawn from the configured, infrastructural and operational complexity of ethnography as an experimental situated practice. As George Marcus suggests, 'experiment today is thus less about writing strategies and more about creating forms that concentrate and make accessible the intermediate, sometimes staged, sometimes serendipitous occasions of distinctively anthropological thinking and concept work' [@Marcus_2014, 400]. The chapter documents  and practically re-enacts a  series of operational queries  --  both in the technical sense of a statement executed by a database and a question posed to a given state of affairs -- concerned with large numbers and technical ensembles.  

[^16]: A recent special issue of the _Journal of Scandinavian Social Theory_ on numbers offers a useful variety of engagements with the status of number. See for instance [@Guyer_2014] on percentages or [@Gerlitz_2014] on 'kudos' ratings.  

While I'm very drawn to attempts to count things and to weave counts into ethnographic writing, during this project I often wondered about how to count things whilst paying attention to their composition. Marcus enjoins ethnographers to 'make forms of processes' [Marcus_2014,401]. The problem is, seen in  a rather literal-minded way, counting is a process that very easily renders the form of a number. An ethnographic sensibility around numbers at a very minimum entails a double counting or re-counting. In the setting I describe (software development, distributed information and knowledge infrastructures),   numbers and counts are site of entanglement between device-specific materialities and collective imaginaries of order, totality, rank, inclusion, boundary and power.  This double sense of numbers as both a calculatively ordered cutting-framing-summing-up form and as materially specific figuring of differences could well be understood in terms of what Lucy Suchman describes as a 'configuration': 

>the device of _configuration_ has two broad uses. First, as an aid to delineating the composition and bounds of an object of analysis, in part through the acknowledgment that doing so so is integral not only to the study of technologies, but to their very existence as objects. And second, in drawing our analytic attention to the ways in which technologies materialize cultural imaginaries, just as imaginaries narrate the significance of technological artefacts. Configuration in this sense is a device for studying technologies with particular attention to the imaginaries and materialities that they _join together_, an orientation that resonates as well with the term's common usages to refer to the conjoining of diverse elements in practices of systems design and engineering [@Suchman_2012, 48].

While Suchman's examples of configuration come from software systems, I'm suggesting that a single number -- 29 million repositories on Github -- could be re-counted as a configuration. The argument is that even the listing, enumerating and counting of things in a single number can be seen as a configuration. Practically, a configured number, if such a curly term is permitted, will need to be counted in ways that hold the two aspects Suchman highlights together. A configured number might be integral to the existence of the 'technology,' in this case the 'social coding' platform Github. At the same time,  that number would, in its counting, call our attention to what is being materialized in terms of an imaginary and all that entails.[^13]

[^13]: What is an imaginary? I will not address that question directly here, but all concepts of imaginary (ranging from Lacan's to the more recent proliferation of imaginaries in humanities and social science [@McNeil_2017]) concern a sense of wholeness or completion that allows formations of identity, inclusion, belonging and otherness to take shape. For an example of the concept of imaginary in an account of software and coding, see [@Kelty_2005]. 

## Re-counting a capital number on  a platform

On Github, two numbers have cardinal and indeed capital importance:  the number of people using the platform, and the number of different code repositories stored there.  I deal in this chapter only with the things -- the repositories -- not the people.   The  counting I undertake will focus on problems of copying, duplicating, and imitation of things -- repositories --  that render large numbers a moving substrate.

Software developers (coders, hackers, geeks, software engineers, software architectures, programmers, scientists, etc.) turn to Github to find, deposit, discuss, collaborate, publish and tinker with code. Across a gamut of practices, rituals, organisational forms and inventions, they rely on an underlying set of protocols, tools and workflows focus on the problem of versions and versioning of code called `git` [@Straube_2016].  Github, started in in 2007, epitomises and has indeed been central -- as the suffix 'hub' suggests -- to a mass of configurational events associated with the development of big data practices in association with large technical ensembles. Like many social media platforms,  Github has grown tremendously in the last ten years  to  around 55 million software projects (March 2017;  the current total of the 29 million that I focus on). Its growth flows from a variety of processes that are difficult to summarise or classify partly because the actors, topics or domains of coding are diverse, and partly because much of what flows through Github is both technically and socially 'innovative' in the sense that Bruno Latour uses the term: '"innovative" means that we do not know the number of actors involved in advance' [@Latour_1996, 72]. (Again, a problem of counting appears here, albeit only in the limited form of 'not knowing in advance.')

Given this massive confluence  of coding, configuring, copying, collaborating, and much playing around with repositories in sometimes quixotic fashion (as we will see, many repositories are highly transient, and very many have a spam-like quality), Github,  like many contemporary assemblages, seeks to describe itself through large numbers such as '55 million projects.' We might call them  'capital numbers' since they serve a vital function in attracting investment (Github, with a market value of \$2 billion has received several hundred million dollars in venture capture [@Gage_2015]), imbuing the platform with hub-like importance in contemporary techno-economies (see for instance, Quentin Hardy's writing about Github  and 'open everything' [@Hardy_2012]). They also, as I will suggest, directly lend weight to  the injunction to 'tell stories with the [big] data.'  Capital numbers in their sometimes daily changes and updates attest to an investment in being innovative and in open-ended processes of growth.

As Suchman's concept of a configuration suggests, capital numbers combine with cultural imaginaries of work, value, power, and technology in manifold ways.   In June 2014, the Github 'about' page showed a photo of a women working in an urban office location, with lights and professional camera focused on her work. It described '6.1 million people collaborating right now across 13.2 million repositories ... building amazing things together' [@Github_2014a]. In late November 2015, the same page has a slightly more functional description, and the image seems to be of a crowded press conference in China:

>GitHub is how people build software. With a community of more than 12 million people, developers can discover, use, and contribute to over 29 million projects using a powerful collaborative development workflow. [@Github_2015](https://web.archive.org/web/20151216055610/https://github.com/about)

The numbers change over time alongside the composition of the actors involved (women coding and working at Github, a sore point for the company as a co-founder was involved in sexual harassment complaints; Chinese developers using Github, but also political activists, leading to a denial of service attack on the Github, allegedly by the Chinese government). But the mundane yet sublime scale of these numbers (a 'community' of 12 million people?, 29 million software repositories?) certainly form part of an ongoing cultural, organisational and financial capitalisation.

## The litany of sense-making: enumerating the aggregate through events

In 2013, a year in which the injunction to do things with data was impacting social science research funders as well as many people in media, commerce, industry and government,  my aggressively data-analytic ambition with respect to Github  was that I could critically  re-count the capital numbers -- people and repositories -- in some way. I planned to use data analytic methods and infrastructures in all their stacked sophistication to count software projects on Github. In re-counting them, I imagined that I would capture  something of their composition in all its heterogeneity and conformity, and show how their scale was an effect of specific dynamics of imitation. HERE  to combat the onslaught of 'technology -> open -> good' narratives,  and perhaps to suggest that such numbers were highly performative in essence.  This would be a both political and culturally worthwhile since capital numbers are commonly used in the capitalisation of  social media platforms. 

How could I  actually count software project or people? Like many other people, I turned to data published through the Github API (Application Programmer Interface). APIs  were not created as a data resource for social scientists, but for software developers working in convergence cultures [@Jenkins_2004] in which connecting different platforms, devices, sites and practices together using code has become standard practice, APIs have great practical significance [@Bucher_2013]. In Github's case, data published through the API derives from acts of writing and reading code (although not only code -- an extraordinary variety of other documents, texts, images, maps, and other forms of data can be found on the platform).   The data is logged  as developers write code and move  that code in and out of code repositories using the `git` version control system [@Git_2014]. Coding today constructs and take place in increasingly complex associative infrastructures.  Github seeks to figure coding as a social network practice based on a hub.

By convention much API data from social media sites has an 'event' structure that links named actors and named entities to specific infrastructural locations (usually coded as an URL) at a particular time (the 'timestamp'). While not all big data has this timestamp-actor-location-links texture, it is common enough to stand in as typical of the data that big data infrastructures and practices encounter. As Noortje Marres has persuasively argued, these data formats are not 'neutral' or in any way particularly good for social research, since they tend to pre-structure the kinds of engagement that one might have with the data [@Marres_2012a]. Work needs to be done with and against the data format. 

```{r user, echo=TRUE, cache=TRUE}
    library(curl)
    library(jsonlite)
    con2 = curl('https://api.github.com/repos/metacommunities/metacommunities/languages')
    lang = fromJSON(readLines(con2), flatten=TRUE)
    lang
    con3 = curl('https://api.github.com/repos/metacommunities/metacommunities/contributors')
    people = fromJSON(readLines(con3), flatten=TRUE)
    people
    con4 = curl('https://api.github.com/repos/metacommunities/metacommunities/branches')
    branches = fromJSON(readLines(con4), flatten=TRUE)
    nrow(branches)
```
In our/my case -- I'm not sure how to report on the work I did with others in this project -- I wanted to write code, which I like doing, to gather, aggregate, clean, explore, visualize and model what has happening. The code vignette shown above summarises some of this (I return to the configurative events behind this summary below). And a total 'all-the-data' scale, since that is big data practice, and I was interpellated by big data/data science practice. The code vignette shown below gathers the last few hundred events on the  Github timeline, and is  a typical starting point.  

```{r events, echo=TRUE, cache=TRUE}
library(curl)
req = curl_fetch_memory('https://api.github.com/events')
cat(rawToChar(req$content))
```

The two lines in the vignette or `Gist` as it would be called on  Github, fetch the latest public events from the API, and display them. Rather than displaying them big data in practice would feed them all into some datastore awaiting further analysis. 


What would it mean to count things using formatted, API  data? These JSON records conjoin elements together.  The relatively simple  `WatchEvent` on Github shown in the data extract above documents how an `actor` calling themselves `mmemetea` associates as a `Watcher` with a  repository called `azondi`, a software project coordinated by the 'organisation' `OpenSensorsIO`.[^2]  Note that the event also has various attributes -- it is a `public` event, it has a 'payload' (often much more complicated than simply `started`) -- and includes various indexical references or `id`s that link the event to other groups of people, organisations, repositories and images (`gravatar_id`). The intricate  syntax of this data -- many brackets, inverted commas, colons, commas -- attests to a complex social configuration which links actors, actions, places and times in discrete events ordered in time. The discreteness of components in this event - actor, organisation, repository, payload, gravatar, etc. --  attests to the potential for the precise configuration of elements around a given repository to change or rearranged in some way. New actors might be added; relations might appear between entities; the location of entities might shift, and forms of association ('organizations') might subsume or grow out or around all of this. In short, each event in the timeline API suggests another small re-configuration in the totality of elements comprising Github. 

## Imbuing numbers with importance

When our project started in 2012, we were clearly not the only people interested in counting things amidst the massive stream of event data concerning Github. A dataset purporting to contain the whole public event timeline of Github appeared in mid-2012. Ilya Grigorik, a 'Web Performance Engineer' at Google, launched a Github repository `igrigorik/githubarchive` linked to a website `GithubArchive.org` dedicated to archiving all the Github API public event data -- the so-called 'timeline' --  in one place [@Grigorik_2012].  Grigorik, or `igrigorik`not only published all the data  as hourly packages in a cloud-based data store but also copied that data  to  Google's newly launched cloud computing data analysis platform, `GoogleBigQuery`.[^1] The [Github timeline data](https://bigquery.cloud.google.com/table/githubarchive:github.timeline) was listed, along with all the words in Shakespeare and all the US birth name records, as one of three dataset exemplars that people could use to learn about Google `BigQuery` [@Google_2016]. Like the data on GithubArchive itself, the Google `BigQuery` copy of the Github public timeline data was updated hourly. Reaching back to  2011, it logs around 400 million public events.[^20]

[^20]: Slightly complicating matters, the single Github timeline dataset has now been retired on GoogleBigQuery and the data now takes the form of separate tables for each day, month and year since February 2011. More recently, the full contents and histories of over 2 millions  Github repositories have been published as a  GoogleBigQuery dataset [GoogleInc_2016a]. 

The archiving, copying and transformation of Github event data into  big data-ready format drew the attention many people. The data was publicised through several 'Github Data Challenges' (2012-2014). Working to count events in different ways using data analytic techniques and data visualizations, people re-counted events on the Github timeline to in order to tell different 'stories' about Github's millions of repositories and people.   These stories include many different configurations. 'Data challenges,' hackathons and the like presented difficulties for my project of recounting the Github repositories. In some ways, the entries and projects that respond to the publication of the data threaten to supplant or render redundant the efforts of social researchers. They 'socialise' big data in multiple ways, albeit often retaining if not reinforcing aspects of its capitalisation.

For instance,  the 'OpenSource Report Card' (http://osrc.dfm.io/)  or `dfm/osrc` by Dan Foreman-Mackay [@Foreman-Mackay_2014], is a prize-winning use of the timeline data (see Figure \@ref(fig:osrc)). It ingests all the data from the Githubarchive, counting what developers do, when they do it, and using what programming languages. With this data stored, it then builds a predictive model that allows it to both profile a given Github user in terms of the mixture of programming language they use, and to predict who that Github user might be similar to. Here the mass of events in the Github timeline are brought to bear on finding similarities between people, producing numbers and score to suggest similarities in coding work. 

In response to the Github Data Challenge in 2012, people looked for feelings or 'sentiments'  about different programming languages in the timeline data.  Feelings associated with coding were mined by counting emotional words present in comments accompanying the Github events (http://geeksta.net/geeklog/exploring-expressions-emotions-github-commit-messages/). The presence of words in these message can be cross-linked with programming languages in order to profile how different programming languages elicit different emotional reactions. 

Capital numbers were also *nationalised* or regionalised. People  mapped coders and repositories by geographic location. The  mapping of Github contributions by location performed by David Fischer (http://davidfischer.github.io/gdc2/#languages/All) is typical in that it too counts events, but this time puts the emphasis on the geography of the 'top' repositories, coders and their programming languages. As the David Fischer puts it 'this data set contains contributions to the top 200 GitHub repositories during the first four months of 2013 and plots the location based on what the contributor provided' [@Fischer_2013a].  

*Logistic numbers* can be derived from the data streams. People made live dashboards, a characteristic data analytic visual form, for Github. `Octoboard` (http://octoboard.com/) animates changes on Github using the timeline data [@Roussell_2015] (see Figure \@ref(fig:octoboard)). It ornaments the capital numbers with a range of peripheral live enumerations that point to the liveliness of events on Github. It presents a summary of daily activity in major categories on Github – how many new repositories, how many issues, how repositories have been 'open sourced' today. It offers realtime analytics on emotions. Like many other dashboards associated with social media analytics, `octoboard` suggests that the constant change in associations and projects in software development can no longer be known through leisurely rhythms of analysis, but is increasingly framed as  a problem  realtime awareness. 

Looking slightly more widely, the Github timeline data has quickly become a favourite training tool for data mining textbooks books that configure and convey the calculative agencies characteristic of capital numbers. In  *Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More,* Matthew Russell makes use of the Github timeline to demonstrate ways of uses social network analysis to highlight the important nodes and links between repositories and users [@Russell_2013]. Finally and not least for my own projects, academic researchers in computer science and certain parts of management science, GithubArchive and the `BigQuery` publication of Github  has been a boon because it permits relatively easy  study technologically and economically important practices of software development.  Academic researchers in fields such as software engineering do social network analysis in order to gauge productivity, reuse, efficiency and other engineering and management concern [@Thung_2013]. Like the many Github-hosted projects discussed above, they analyse sentiment [@Guzman_2014], collaboration and productivity [@Dabbish_2012], and geography [@Takhteyev_2010].

All of these re-countings enliven, animate, reactivate, localise and qualify the capital numbers. They potentialise the numbers in terms of work, geography, liveness  and further accumulation by summing them up in different ways (realtime status updates and dashboards, networks of associations, geographies of work and affect) commonly found in contemporary data economies and as the outcome of big data practice.  Many of the dashboards, maps, sentiment analyses and predictive recommendations are common in  big data practice, and the fact that people using Github should so readily analyse Github itself using big data infrastructures such as GoogleBigQuery and other analytic devices is hardly surprising. Coders and software developers are, after all, key workers in the ongoing transformation of systems of controls and configuration associated with  big data. 

## Acts of imagined accumulation

Given this vortex of data-challenged work around the Github data, which I interpret as both symptomatic of  big data practices and as a set of parallel 'storyfications' of the 'datafication' of Github, where would an ethnographic sensibility intervene? Is there any place or point at which it might engage with the code-infrastructure oriented practices associated with flows of data? Given my -- and indeed 'our', since this project was a team effort, including statisticians and social theorists --  propensity or somewhat unwitting interpellation as a white, middle-class, middle-age, relatively technically over-skilled male social researcher long-drawn into information infrastructural transformations, the chances of an ethnographic attentiveness to differences, slippages, experiential ambivalences and ambiguities actually having an chance of purchase were rather slim. Could writing queries for GoogleBigQuery GithubArchive dataset, developing data analytic and visualizing code in `R` and `python`, programming languages commonly used in the  big data practices actually generate any auto-ethnographic resonance? 

The basic capital number of the repository count was usefully grounding. Technically, the number is relatively easy to approximate in the GithubArchive data. A simply GoogleBigQuery produces the number in roughly 2.0 secs having processed around 40Gb of repository URLs:

```{r repo_count, cache=TRUE, echo=TRUE, warnings=FALSE}
library(bigrquery)
query1 = "SELECT count(distinct(repo.url))
    FROM 
    [githubarchive:year.2011],
    [githubarchive:year.2012],
    [githubarchive:year.2013],
    [githubarchive:year.2014],
    [githubarchive:year.2015]"
    repo_count = query_exec('metacommunities', query = query1)
```

The result: `r repo_count`

How could we re-count such capital numbers in order to both further highlight their dynamic composition through flows of association and to highlight some of the highly reactive, imaginary boundary work they might do as the materialize imaginaries of totality, globality or infrastructural control? Could configurative numbers in both their compositional and imaginary-materialising multiplicity be counted? Whilst there is a much configurational work involved even in beginning to count things heterogeneously on an infrastructural scale, I found myself drawn in two directions, one concerned with transience of configurations and the other with their imitative composition. Both concerns -- transience and imitation -- are not highlighted in any of the other re-counts of the GithubArchive data, but they only became salient to me amidst many other failures to find any interesting story, stable signal or statistical regularity in the Github timeline data.[^40]

[^40]: Nearly all of our attempts to make sense of the data can be found in the Github  repository [https://github.com/metacommunities/metacommunities](https://github.com/metacommunities/metacommunities). The scripts, plots and pieces of writing found there, however, are not highly ordered. The key results lie scattered across different branches of that repository. The somewhat tangled, messy aggregate of materials there attests to our strenuous but often incoherent efforts to find valid statistical regularities in a dataset characterised by tremendous heterogeneity and messiness. 


When we count people or things (such as software project), the working assumption is that they have some duration and substantial presence. This assumption, however,  does not hold very true at the scale of large data-streams, where, like sub-atomic particles in a collision, people and things flash in and out of existence quite rapidly and unpredictability. Frustrated by our seeming inability to find anything in the GithubArchive data apart from the obvious importance of well-known software projects such as `linux` or `android`, we were forced to accept transience and ephemeral visibility as a common mode of existence on Github. The transient visibility of people and things in the data takes many forms.  The vast majority of repositories on Github are very short-lived. They attract one or two events. A great proportion of events in the timeline data were absorbed into ephemeral repositories (if that is not too great a contradiction in terms).  Millions of repositories flash into visibility on the timeline for a brief period before falling back into obscurity. Yet they remain in the capital number. The diagram shown in the Figure \@ref(fig:power_time) encapsulates this imitative flux. On the left hand side, millions of repositories receive less than five events during the 18 months. On the right hand side, less than 50 repositories receive more than a thousand events. A similar pattern appears in the other capital number: while some 'people'  emit many thousands of events, others only trigger a few.[^5] Already, then, the capital number of repositories on  Github takes on a different composition when viewed from the perspective of duration. In the number '29 million,' 22

The massive asymmetry between the relatively few long duration repositories and the vast mass of transient, abandoned repositories is definitely not another corroboration of 'the long-tail' distributions that social media exponents such as Clay Shirky and Chris Anderson began discussing more than a decade ago and that may have had a large part to play in many of the intensely individualizing tendencies of big data analytics, targetted advertising, predictive feeds and recommendations. (For instance, one reason that Amazon.com stocks so many obscure products, is that the long tail of sales of these items is  potentially more important than the sales of a smaller number of best-sellers [@Byrnnjolfsson_2006; @Anderson_2009].)   Rather than the long tail of coding, or a scale-free network of code-hubs on Github (itself a hub in networks of code-related infrastructure), or even simply waste, noise or something to be discarded, we might trace the working of associative processes through highly skewed distribution of events. I can only give a brief indication of this here. For instance, counting just the unique names of repositories in the timeline data, counting how often and when they appear as event in the data stream begins to suggest something about the composition of the capital number of repositories. Almost two thirds of the repositories in Github inject only one or two events into the timeline data stream ever. This means that 20 million of the 30 million repositories are just flashes in the timeline data appearing as one or two events. 

```{r fork_count, echo=FALSE, cache=TRUE, warnings=FALSE}
library(bigrquery)
query = "SELECT count(*) cnt
FROM [githubarchive:year.2011], [githubarchive:year.2012],[githubarchive:year.2013],[githubarchive:year.2014], [githubarchive:year.2015]
where type=='ForkEvent'"
df = query_exec('metacommunities', query=query)
f = round(df[1], 2)
```


```{r event_count, echo=FALSE, cache=TRUE, warnings=FALSE} 
library(ggplot2)
library(bigrquery)
library(dplyr)
query2 = "SELECT type, count(type) cnt FROM
    [githubarchive:year.2011],
    [githubarchive:year.2012],
    [githubarchive:year.2013],
    [githubarchive:year.2014],
    [githubarchive:year.2015]
    GROUP BY type
    ORDER BY cnt DESC"
df = query_exec('metacommunities', query=query2)
g = ggplot(df, aes(x=type, y=cnt)) + geom_bar(stat='identity') 
g + coord_flip()
social_events = df %>% filter(type %in%  c('WatchEvent', 'IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent', 'ForkEvent')) %>% summarise(sum(cnt))
proportion = round(social_events/sum(df$cnt)*100, 0)
```

What are these events? The event of creating a named repository is primary, followed by the event of pushing (putting something into a repository -- a 'commit'), and then  act of copying ('forking') another repository. People 'fork' other repositories frequently. If we just count the event of creating a repository by copying or forking it, more than half all repositories are copies of existing repositories (`r f `). If every ForkEvent on Github creates a new repository, more than half of the repositories are direct copies. Acts of copying occur on many scales and at various levels of infrastructural and associative complexity. This copying is vital to the 'sharing' practice of Github coding. Of the several hundred million events in the timeline dataset, approximately `r social_events` or `r proportion`% of all events in the Github timeline data arise from copying, watching, commenting on or otherwise observing other repositories.(See the WatchEvent, ForkEvent, PullRequestEvent,and IssueEvent counts in figure \@ref(fig:event_count).)    

## The growth of associative imitation

```{r android_associations, echo=FALSE, cache=TRUE}
    library(bigrquery)
    library(ggplot2)
    library(readr)
    library(dplyr)


    construct_query <- function(query_term) {
        query_full = gsub(pattern='\\n', replacement =' ', x=paste("SELECT
        repo.name,
        created_at,
        COUNT(type) AS cnt
        FROM
        [githubarchive:year.2011],
        [githubarchive:year.2012],
        [githubarchive:year.2013],
        [githubarchive:year.2014],
        [githubarchive:year.2015],
        TABLE_DATE_RANGE([githubarchive:day.], TIMESTAMP('2016-01-01'), TIMESTAMP('2016-06-31'))
        WHERE
        type= 'ForkEvent'
        AND LOWER(repo.name) CONTAINS('",
        query_term,
        "')
        GROUP BY
        repo.name,
        created_at,
        ORDER BY
        created_at,
        cnt DESC", sep=''))
        return(query_full)
    }
    

    fetch_data <- function(query_term, pages_to_fetch=4, force_refresh=FALSE) {
        query_full = construct_query(query_term)
        file = paste('data/', query_term, '_forks.csv',sep='')
        if (file.exists(file) && force_refresh == FALSE) {
            df_and = read_csv(file)
        } else {
            df_and = query_exec('metacommunities', query= query_full, max_pages= pages_to_fetch )
            write_csv(path=file, x= df_and)
        }
        head(df_and)
        nrow(df_and)
        return(df_and)
    }


    plot_forks <- function(df_and, query_term, fork_count_cutoff, binwidth, without_query = FALSE){
        top_forks = df_and %>%group_by(repo_name) %>% summarise(evt_cnt = n()) %>% filter(evt_cnt>fork_count_cutoff) %>% arrange(desc(evt_cnt))
        nrow(top_forks)
        if (without_query){
            to_plot= df_and[df_and$repo_name %in% top_forks$repo_name[-1], ]
        } else {
            to_plot = df_and[df_and$repo_name %in% top_forks$repo_name, ]
        }
        nrow(to_plot)
        binw = binwidth
        g = ggplot(to_plot, aes(x=as.Date(created_at),   fill=repo_name)) + geom_line(aes(y = ..count..),  color='black',size=0.2, alpha=0.3,  stat = "bin",position='stack', binwidth=binw) +  theme(legend.position='blank') + geom_area(aes(y = ..count..), stat='bin',alpha=0.4,  binwidth=binw)  + scale_colour_brewer(type='div', palette='RdYlGn') + ylab('Fork counts') + xlab('Date of Fork')

        if (without_query){
            g + ggtitle(paste('Forks associated with the name `', query_term, '` alone', sep=''))
            } else {
            g +  ggtitle(paste('Forks associated with the name `', query_term, '`', sep=''))
            }
    }

    query_term = 'android'
    fork_count_cutoff = 50
    binwidth=10 
    pages_to_fetch= Inf
    df_and = fetch_data(query_term, Inf, TRUE)
    plot_forks(df_and, query_term, fork_count_cutoff, binwidth, FALSE)

```


While it is hard to grasp the texture of imitative processes in event data, they figure deeply in the composition of the capital numbers. We can observe some  localised aspects  of propagation of imitation by going beyond the formatted data of the API or  GithubArchive data. Much social media platform data relies heavily on unique names (hence, for instance, Facebook's insistence that every user has a single unique identity). Yet the associative play of names (of people and things) attests to processes of associative imitations that continually   In the years 2012-2015, both mobile devices and user interfaces were being intensively transformed in complex ways (e.g. the growth of apps, and the transformation of webpages from static HTML-formatted text to thoroughly dynamic script-driven surfaces composed for many elements). While these transformation were not directly the work of 'big data' practices, they are intricately connected to it by virtue of the proliferation of devices, the deeper integration of networks of association they implement and not least, the changes in the texture of contemporary experiences of interfaces.    Both Figure \@ref(fig:bootstrap_assoiation) and \@ref(fig:android_associations) attempt to figure something of this proliferating-integrating process of change.  Both figures  count imitations in the form of copying code, but in ways that go beyond the formal copying mechanisms offered by Github itself (for instance in the 'Fork' button that appears on the top right hand side of any repository on Github).

```{r bootstraph_association, echo=FALSE, cache=TRUE}
    query_term='bootstrap'
    df = fetch_data(query_term, 4)
    plot_forks(df, query_term, 10, 10, FALSE)
```

We might call this form of composition, as it configures capital numbers, *stacked associative imitation.* Associative imitation appears in two different ways in these figures. The broad bands of color rippling horizontally  across the base of the figures graph the counts of copies being made each day on Github of popular repositories such as `android` or `bootstrap` using the Fork action. (In forking, the repository name remains the same.)   But the much more dense striations running above the base ribbons,  seen for instance in Figure \@ref(fig:android_associations), count repositories whose names combine the base repository (e.g. `android`) but vary it in some way. These repositories associate with the base repository, but diverge from it in a multiplicity of ways. A repository may, for instance, relate to the popular  `bootstrap` repository yet combine it with a range of other platforms and devices such as `android` or `jQuery`. 

In the striated zones of associo-imitation plotted in the figures, it is possible to count something that is neither the long tail nor a link-based network of affiliations.   The cross-ply of repository imitations identify the average everydayness of coding work with forms of associative investment and affective identification that range across many different repositories.  In this work,  repositories act as high visibility markers around which waves of hybridisation stack up. Unlike the capital numbers with their total aggregates of people or repositories, the imitative fluxes that bulk out and pad in these diagrams have diverse networks of association.  The implication here is that by counting elements within the capital numbers, we can begin to glimpse the pathways of generative circulation that enliven and literally join together in contemporary technologies.      

## De-capitalising configuration

Densely populated with counts of ephemeral repositories, stacked with associative imitations, the capital number 29 million seemed in the light of these re-counting to be a sieve-like entity, full of differently configured processes. Yet still the count was not exhaustively accounted for in these different counts.     The very act of counting these imitations has a device-specificity, in its reliance on the Github platform API, the GithubArchive timeline dataset and the GoogleBigQuery cloud analytic's platform, and indeed Github itself as the repository of the code and text comprising this article [@Metacommunities_2016]. Unlike the network-tracing approach, it can perhaps take into account the ways in which the platform itself works to shape networked associations. Whenever we count or calculate in any form, device-specific configurations materialise in the numbers.  The expansively contagious nature of imitation inevitably encounters device-specific configurations.       

```{r event_proportions, echo=FALSE, cache=TRUE}

    query_evt = "SELECT
        count(type)  
        FROM
        [githubarchive:year.2011],
        [githubarchive:year.2012],
        [githubarchive:year.2013],
        [githubarchive:year.2014],
        [githubarchive:year.2015]"
    total_events = query_exec('metacommunities', query = query_evt)[1]

    query_top_repos = "SELECT
        TOP(LOWER(repo.name),30000),
        COUNT(*) AS cnt
        FROM
            [githubarchive:year.2011],
            [githubarchive:year.2012],
            [githubarchive:year.2013],
            [githubarchive:year.2014],
            [githubarchive:year.2015],
        "
    total_events
    top_df = query_exec('metacommunities', query = query_top_repos)
    top_prop = sum(top_df$cnt)/total_events * 100 
```

One sense of how numbers embody device-specific configurations can be seen by examining high event-count repositories on Github.  The top 0.1% of repositories attract around `r round(top_prop)`% of all events during 2011-2015. (The query de-capitalises the repository names for the purposes of aggregation (e.g. a repository called `DotFile` will be counted along with `dotfile`)). Given the flattening of the names to lower case produced by this query, we cannot readily see how those events are distributed. But these highly eventful repositories, which only comprise a tiny percentage of the 29 million on  Github, absorb many events. What happens in these high event count repositories? 
 
Several major traits appear in these high event-count repositories. Many events come from off-platform. Github, despite its own figuration as a hub where coding is done socially, functions as an element in wider configurations.   For instance, the repository `eclipse.platform.common` accounts for almost 2 million events in the timeline data. A single repository attracting two million events (or almost 1% of the total event count in the timeline data)  suggests something highly significant in the geography of coding work. Perhaps the fact that `eclipse` is itself part of the Eclipse Foundation,  'an amazing open source community of Tools, Projects and Collaborative Working Groups' [@EclipseFoundation_2016] with almost a thousand of its own projects, might help explain the large number of events. More significantly, the high-frequency event traffic in `eclipse.platform.common` is an example of how Github itself functions as part of a configuration.  The `eclipse` repositories are not actively developed on  Github. They are mirrored  or themselves copied from the hundreds of `git` repositories found at (git://git.eclipse.org)[git://git.eclipse.org]. Like many other significant repositories (`linux`, `android`, `mozilla`, `apache`), high event counts often configure Github itself as part of a wider network of relations. 

Conversely, even where repository events do originate on Github, many of them concern Github or  technical configurations closely associated with Github.   For instance, the list of names of high-event count repositories centres on a few highly repeated names such as `test`, `dot`, `sample` and `try_git`. For instance, the many repositories that contain the term `dot` as in `dotfile` or `vimdot` suggest that Github repositories act as stores for the settings and configurations specific to coding work (e.g. `vimdot` repositories hold customised settings for the popular `vim` text editor).  Terms like `test`, `hello`, and `try` or `demo` also stream through this set of repository names. These repositories often attest to non-code uses of Github and sometimes (for instance, in the case of `KenanSulayman/heartbeat` with its several million commit events) to surprising or experimental re-purposings of the Github platform. 

Do configuration-oriented practices around Github affect the capital numbers and the value and importance they are meant to convey? Configuration is, as science and technology studies scholars have emphasised, vital to the very existence of technologies.  If many of the 29 million repositories on Github function as elements in the configuration of coding work rather than as bodies of code in their own right, then any account of what happens cannot assume that analysis of Github data primarily concerns 'how people build software' [@Github_2015]. Or, put more constructively, it suggests that much of what counts as software on Github -- code repositories -- in actuality concerns the arrangements that people make with each and themselves in order to work with software. In this respect, the capital numbers are always also configurative numbers. There is no work on software without the 'how' of building software, the configuration work.   


In this context, too, our  -- the five researchers directly involved in the research -- work on the Github API, GithubArchive datasets and the GoogleBigQuery platform was full of configurative events, themselves tending to slightly increase the capital numbers (although we only added one repository to Github). The repository for this project contains thousands of lines of code written in Python, R and in specialized languages such as SQL (Structured Query Language)[@Metacommunities_2016] as well as tens of thousands of lines of text distributed across `r nrow(branches)` branches of the repository. According to the  Github API, user `rian39` added around 10 million lines of code and deleted 8 million, leaving a net contribution of around 2 million lines to the `metacommunities` repository during the years 2012-2016 [@Metacommunities_2016a]. While those numbers might be read as attesting to extraordinary levels of code productivity, they actually include many lines of data stored in the Github repository, alongside code, drafts of documents and configuration related information. The deletion of 8 million lines suggests that some of the main activities in the repository were changes to do with cleaning up, tidying and re-arranging files and documents in the repository.     

```{r bigquery, cache=TRUE, echo=FALSE}
    library(jsonlite)
    library(ggplot2)
    library(dplyr)
    j = stream_in(file('data/jobs.json'), flatten=TRUE)
    s=sum(na.omit(as.numeric(j$statistics$totalBytesProcessed)))/1e12
    print(s)
    bytes = as.numeric(j$statistics$totalBytesProcessed)
    dates = as.Date(as.POSIXct(as.numeric(j$statistics$creationTime)/1000, origin="1970-01-01"))
    df= data.frame(date = dates, vol = bytes)
    gbday = df %>% na.omit  %>% arrange((date)) %>% mutate(gbcumul = cumsum(vol/1e9)) %>% group_by(date) %>% mutate(job_count = n())
    head(gbday)
    dl = data.frame(x = c(as.Date('2016-07-31'), as.Date('2013-08-13')), y =c(80000,13500) , labs = c(paste(as.character(s), 'terabytes'), '13500 jobs ran overnight')) 
    g= ggplot(gbday, aes(x=as.Date(date), y=gbcumul)) + geom_line()  + ggtitle(paste(s, ' terabytes of BigQuery processing in counting a capital number'))+ ylab('Gb of data (cumulative)') + xlab('')
    as.character(s)
    g = g + geom_text(data=dl, aes(x=x, y=y, label=labs, group=NULL))
    g
```

Perhaps this level of contribution activity to a repository suggests something important about configurative numbers in big data practice. Most of the 10 million lines added and 8 million lines deleted were configuration-related work as we -- I in particular -- sought to make sense of capital numbers such as 29 million, and tried to differentiate that sense-making from the flood of data challenges, hackathons and  other usages of the GithubArchive data that were also offering geographical, sentiment, logistic and temporal narratives about 'how people build software.  In the process of trying to re-count the capital number, we 'built software' that generated 400 or images, 100 or so data files, and around 1000 other files. More than 22000 queries on GoogleBigQuery took place. Figure \@ref(fig:bigquery) shows something of the flow of data involved in this effort. By mid-2016, 80 terabytes of Github data had been searched, queried, tallied, counted, sorted and arranged according the job logging supplied by the Google Compute Platform. This include one day in August 2013 where 12,000 jobs executed. It appears as a steep ascent on the left hand side of the graph. Traversing 40 terabytes, and generating a bill of around \$US2000, the events of that day  triggered a 'reaching out' from Google Compute marketing department who were interested in our 'use case.'  Why write so much code, process so much data, create so many figures and images, and indeed organise all of this work in the sometimes maddeningly precise version control system of a `git` repository on Github? 

## Conclusion

Suchman's suggestion that configuration always concerns both composition of elements and materialising imaginaries seems particularly resonant here.  It takes work to get contemporary digital data and associated large numbers to do something other than add to the count of capital numbers and their platform-centred aggregates.  Configurative numbers, I have suggested, is one term for the versions of enumeration that result from  re-countings that seek to identify the composition of capital numbers, and to follow some of the imaginaries aggregating in them.    We saw how that re-counting points to many attempts to storify data flows, to  associative-imitative fluxes, and to very extensive configuration work that saturate the data stream with links to what happens elsewhere. 

Where does this leave 29 million repositories? Halved by all the repositories with no more than a single event, halved again by all the repositories that are a copy of other repositories, hollowed out by all the repositories where people do not build software but use the repository for some other purpose (as a backup site for instance), millions of code repositories   remain.  Github released the full source code of 2 million repositories on GoogleBigQuery in mid-2016, perhaps reflecting the reality that the 30 million or so other repositories were not going to add new stories to the platform. 

## References
