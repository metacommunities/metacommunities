# Metacommunities of practice in the code-sharing commons

## Adrian Mackenzie (Lancaster), Matthew Fuller (Goldsmiths), Andrew Goffey (Nottingham), Richard Mills (Cambridge), Stuart Sharples (Lancaster)

December 2014

## Topics of the paper online data, analytical and presentational tool
·  transmit best practice in use of such tools for social science;
·  show how the tools can be used to test social-science theories; and
·  suggest possible improvements/innovations in the tools to help integrate analytics and open-source data tools in general into the teaching/learning community

## ideas for esrc dragons den paper

- relationality is the potential for singular effects of qualitative change to occur in excess 
||  over or as a supplement to objective interactions.

 -- we didn't do that; but we could use this to identify organisations who we could work with 
- embedding findings in the field
- computer scientists and statisticians as buddies - tensions around that; 
- newbies to the domain -- qualitative and quantitative thing -- voluntary naivete
 - how the api shapes what you can find out -- our attempts to find stuff out; failure to address the infrastructures that produce data
- do you need a lab? there is a need for combining critical perspectives and capacity to do analysis; how do scientists work with their devices to modify them;  uncomfortable message for esrc --- you have to get geeky;
- what we couldn't do -- are we competitors in the fields of github data challenges? william bunge using quantitative data to show inequality; -- conjunctions; lackadaisical solidarity; commons with potential forms of solidarity; 
- what we did with the namespace -- messy data that works against the api -- other examples of working against the api?

##quotes to use



## Overview

>A metacommunity is a set of interacting communities which are linked by the dispersal of multiple, potentially interacting species [@Leibold_2004].

>One of the areas that being most dramatically shaken up by $N = All$ is the social sciences. They have lost their monopoly on making sense of empirical social data, as big-data analysis replaces the highly skilled survey specialists of the past. ... More important, the need to sample disappears [@Mayer-Schonberger_2013, 30].


The 'Metacommunities of practice in the code-sharing commons' project was an $N=All$ style data analysis. By $N=All$, we mean that it sought to make use of all the publicly available data in the domain of research. The title of the project is a slightly loopy one, but relates to this $N=All$ approach in important ways. First of all, the domain of the research was software, code, and coding practices as seen in public software repositories. We focused almost solely on Github.com, the leading public code repository today. We could have done comparative work with other platforms such as GoogleCode or SourceForge, but working with all the data from Github was substantial enough. We did not sample code and coding practices on Github. We sought to analyse them in their entirety precisely because we wanted to look for evidence of the _commons_ in code. Our guiding theme of 'meta-community' was meant to open up a different perspective on the flow of code across platforms, devices, apps and applications. This perspective would contrast with much of the existing social science that focuses right down on exemplary cases (often by doing fieldwork with particular communities of software developers such as Debian [@Coleman_2012]), and diverge also from the extensive work done by computer scientists and software engineers who do look at the aggregate phenomena but tend to focus on issues of productivity, reliability or code quality on particular projects (often by using software repositories as their data source [@Godfrey_2012]). In contrast with both the existing social science and the computer science, we were looking for transverse flows and connections between different parts of the seething pool of software development. More importantly, we were treating the repositories in aggregate as the form to be analysed, rather than seeing them as a site for sampling of independent, representative cases. The overarching goal was something like what ,  as Viktor Mayer-Schőnberger and Kenneth Cukier suggest in _Big Data_: 'using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information' [@Mayer-Schonberger_2013, 27].

## Research objectives and their practicalities

Our research objectives in order of priority were:

1. Using datasets generated from publicly accessible software code repositories and programmer question and answer (Q&A) sites, to explore ways of  tracking programming practices as they move through digital economies/network media cultures.
2. To develop an empirically rich and reusable database of evidence describing the extent and diversity of code sharing practices across a comprehensive range of open-source software projects during the period 2008-2012.
3. To explore and document how data analytic techniques including visual data exploration, statistical machine learning and predictive models can be re-purposed to focus on questions of practice that are normally seen as the province of qualitative case-studies.
4. To investigate whether analytical and methodological problems in the social sciences often framed in terms of the 'empirical crisis of sociology' [@Savage_2009]  or the dominance of social sciences by the 'general linear model of reality' [@Abbott_2000] can be addressed through a  re-conceptualisations of what it means to track practices [@Latour_2012].
5. To demonstrate reproducible social scientific research in the form of tightly integrated 'virtual machine' distribution of data, analysis, code, text and visualizations.

In the course of the project, which ran for around 14 months overall, we did not meet all of these ambitious objectives. We did however address all of these objectives in various ways, ranging from using large public API (Application Programmer Interface) datastreams, using big data analytic tools such as Google BigQuery, and working with various analytic techniques coming from social network analysis, and statistical learning. As well, we worked extensively with the reproducible research tools and platforms in order to test how and to what extent reproducible research can be conducted using tools available in the wild rather than constructed as a bespoke or purpose-built platform for data analysis. As usual, we altered and reoriented some objectives, and curtailed others in order to keep the research in movement through its different phases. 

## Using public datasets

The metacommunities concept, with its focus on dispersion, on multiple 'species' (this could be understood in various ways in our domain), and on potential interactions, suggested a need to work with the data in aggregate. While we ended up focusing almost all of our efforts on the data that relates to the Gibhub software repositories, we also made some of public data dumps of the popular Q&A site [StackOverflow.com](StackOverflow.com) to help identify important processes of interaction between developers. 

We faced some fairly profound practical choices in working with the Github data. We could harvest data from Github's extensive APIs (see <https://api.github.com/>):

    
        {
      "current_user_url"
      "current_user_authorizations_html_url"
      "authorizations_url"
      "code_search_url"
      "emails_url"
      "emojis_url"
      "events_url"
      "feeds_url"
      "following_url"
      "gists_url"
      "hub_url"
      "issue_search_url"
      "issues_url"
      "keys_url"
      "notifications_url"
      "organization_repositories_url"
      "organization_url"
      "public_gists_url"
      "rate_limit_url"
      "repository_url"
      "repository_search_url"
      "current_user_repositories_url"
      "starred_url"
      "starred_gists_url"
      "team_url"
      "user_url"
      "user_organizations_url"
      "user_repositories_url"
      "user_search_url"
    }

As this list shows, we were facing several dozen different access points to what was happening on Github. The data available through the APIs varies in volume, and rate-limits apply to many of the API points of access, but in general we were faced with an abundance of data, of varying duration. The problem was more one of being spoiled for choice. 

Even given this abundance of data, several other major viable alternatives presented themselves. This may not be typical of all social media data, but in the Github case, Github API data from the `event` access point has been archived in a public archive <http://www.githubarchive.org> by a Google employee named Ilya Grigoik for the last year or two [@Grigorik_2012]. This archive was published after we submitted our grant application, but as soon as we discovered it in early 2013, we seriously considered using it because it does not suffer from the rate-limits that Github's own APIs impose. Initially, GithubArchive only reached back to early 2012, but now it extends back to February 2011, so offers a timeline datastream of every public event on Github since then. This dataset can almost be wrangled on a single machine. It totals around 179 Gb compressed, and consists of a file for every hour since early 2011. As is often the case in software cultures, the code that runs GithubArchive can itself found on Github as a `git` repository: <https://github.com/igrigorik/githubarchive.org>. A quick glance at that repository shows that GithubArchive.org feeds directly into another major Github data resource, the Google BigQuery data analytics platform, which hosts the full GithubArchive dataset at [GithubArchive.timeline](https://bigquery.cloud.google.com/table/githubarchive:github.timeline?pli=1). On Google BigQuery (GBQ hereafter), the formatting of the event data is quite different. The data is stored in a single huge table, with almost 200 columns and, at the time of writing, around 280 million rows.  This massive table can be queried using an SQL (Structured Query Language) dialect. 

While we did use the Github APIs, and wrote scripts that allows us to move back past the February 2011 limits of the GithubArchive data, having the full `events` timeline in both the compressed forms of hourly files from GithubArchive and in the GoogleBigQuery table made any efforts to construct our own dataset from scratch seem somewhat futile. But working with either the GithubArchive or the GoogleBigQuery data brings its own problems. GithubArchive is in around 30,000 files that need to be read or loaded into memory in order to work with them. Even using GoogleCompute platform, processing those files takes takes many hours. We did process all the files several times, in particular in pursuit of a full description of the terrain covered by the 12 million or so code repositories on Github, but the result of the processing were new database summaries of the Github data (for instance, we build a [Redis](http://redis.io) database to give us quick access to the main attributes of all the repositories, and all the user attributes whenever we needed it). The problem with these full dataset traversals is that each time they are run, they produce a new summary or synoptic dataset, but a slight change or variation in the information needed for a particular analysis means running all the scripts again. This is not a good workflow.

Most of our daily analyses were constructed then using GoogleBigQuery. As mentioned above, the advantage of this platform is that it allows SQL-style queries to be run against a large, albeit not quite $N=All$ Github dataset. On the one hand, these queries can be done interactively through a website interface (especially useful for incrementally constructing queries), but on the other hand, the nature of the  GithubArchive stored on GoogleBigQuery means that queries are quite difficult to construct. 

```{r schema, results='asis', echo=FALSE, warnings=FALSE, messages=FALSE}

    library(xtable)
    schema = read.csv('~/metacommunities/data_analysis/github_data_infrastructures/googlebigqueryschema.csv', header=FALSE)
    colnames(schema) = c('field', 'type')
    schema$type = tolower(schema$type)
    type_table = table(schema$type, dnn='data types')
    datatypes = xtable(type_table, label = 'tab:gbq_datatypes', caption = 'Columns in GithubArchive on GoogleBigQuery')
    print(datatypes) 
```
As Table \ref{tab:bgq_datatypes} shows, 75% of the fields in the GoogleBigQuery dataset are string, around 20% are numerical count data, and the remainder are boolean or True/False values. But for any given event, many of these fields are likely to be empty because the  GoogleBigQuery has been constructed to accommodate every conceivable event type without consideration of the distribution of events. 


```{r events, results='asis', echo=FALSE, warnings=FALSE, messages=FALSE}
    library(xtable)
    events = read.csv('~/metacommunities/data_analysis/github_data_infrastructures/github_eventtypes.csv')
    event_types = xtable(events, label='tab:gbq_eventtypes', caption= 'Event types on Github Events API')
    print(event_types)

```

[HERE] == show the event types, and point out how unevenly distributed they are; and how much missing data there is;


## Conclusion

- how Github differs from much more complicated twitter-based analysis
- why Github matters as a social research topic
- 
## References


