{
 "metadata": {
  "name": "",
  "signature": "sha256:6a9a8265b996243c9d8a71db9ebb089ab5d99c0655d0a222fb66f7a3174a992a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Validate topics of repos\n",
      "\n",
      "The repository topics are generated by text matching. But that creates a huge amount of noise in the classifications.\n",
      "Is it possible to build classifiers that use some training data for each topic to clean that up a bit?\n",
      "\n",
      "Here I use a Bernoulli Naive Bayes to clean up a given domains. It does mean providing a training set for each domain. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import redis\n",
      "import pandas as pd\n",
      "import re\n",
      "import nltk.stem\n",
      "import re\n",
      "import collections\n",
      "import nltk\n",
      "import numpy as np\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import sklearn.naive_bayes as nb\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Treat the names as source of features to be extracted by cleaning and splitting, etc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "red = redis.Redis(db='1')\n",
      "keys = red.keys('*')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "repo_keys = [k  for k in keys if k.find(':') == -1]\n",
      "repo_keys"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "['documents',\n",
        " 'data_statistics',\n",
        " 'api_sdk',\n",
        " 'programming_language',\n",
        " 'compilers',\n",
        " 'web_frontend_javascript',\n",
        " 'science',\n",
        " 'integration',\n",
        " 'books_manuals',\n",
        " 'packages',\n",
        " 'git',\n",
        " 'webpages',\n",
        " 'devices',\n",
        " 'policy_law',\n",
        " 'gaming',\n",
        " 'servers',\n",
        " 'tests',\n",
        " 'internet_protocols',\n",
        " 'databases',\n",
        " 'geo_gis',\n",
        " 'moocs',\n",
        " 'webframeworks',\n",
        " 'spam',\n",
        " 'css',\n",
        " 'ides',\n",
        " 'images',\n",
        " 'dotfiles',\n",
        " 'machine_learning',\n",
        " 'social_media',\n",
        " 'icons_fonts',\n",
        " 'editors']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Create a training set for domain repos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "domain = 'tests'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_domain_training_set(domain):\n",
      "    sc = red.srandmember(domain, 2000)\n",
      "    # sc_terms = [re.sub('[\\d*_\\W*]', ' ', s.split('/')[1]).lower() for s in sc]\n",
      "    sc_terms = [re.sub('[\\d]{2,}|[_\\W*]', ' ', s.split('/')[1]) for s in sc]\n",
      "    sc_terms = [' '.join(re.findall('[A-Z][a-z]*', t)).lower() if re.match('[A-Z]', t) else t.lower() for t in sc_terms]\n",
      "    url = ['http://github.com/'+s for s in sc]\n",
      "    training = pd.DataFrame({'repo':sc, 'terms':sc_terms, 'url':url, 'is_'+domain:True})\n",
      "    training.to_csv('data/'+domain+'_repos.csv', index = None)\n",
      "    training.to_csv('data/'+domain+'_repos_training.csv',  index = None)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create_domain_training_set(domain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# after categorising 2000 repos by hand!\n",
      "# note that it the csv will now be call x_training.csv\n",
      "training_df = pd.read_csv('data/{0}_repos_training.csv'.format(domain), header=0, sep='\\t')\n",
      "# training_df['is_{0}'.format(domain)] = training_df['is_{0}'.format(domain)].str.lower()\n",
      "training_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>repo</th>\n",
        "      <th>terms</th>\n",
        "      <th>is_editors</th>\n",
        "      <th>url</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> drupalprojects/views_slideshow</td>\n",
        "      <td>   views slideshow</td>\n",
        "      <td> False</td>\n",
        "      <td> http://github.com/drupalprojects/views_slideshow</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>                asael2/satenspr</td>\n",
        "      <td>          satenspr</td>\n",
        "      <td> False</td>\n",
        "      <td>                http://github.com/asael2/satenspr</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>      sepulworld/puppet-mongodb</td>\n",
        "      <td>    puppet mongodb</td>\n",
        "      <td> False</td>\n",
        "      <td>      http://github.com/sepulworld/puppet-mongodb</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>     rainux/vim-desert-warm-256</td>\n",
        "      <td> vim desert warm  </td>\n",
        "      <td>  True</td>\n",
        "      <td>     http://github.com/rainux/vim-desert-warm-256</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>                    ten0s/emacs</td>\n",
        "      <td>             emacs</td>\n",
        "      <td>  True</td>\n",
        "      <td>                    http://github.com/ten0s/emacs</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "                             repo              terms is_editors  \\\n",
        "0  drupalprojects/views_slideshow    views slideshow      False   \n",
        "1                 asael2/satenspr           satenspr      False   \n",
        "2       sepulworld/puppet-mongodb     puppet mongodb      False   \n",
        "3      rainux/vim-desert-warm-256  vim desert warm         True   \n",
        "4                     ten0s/emacs              emacs       True   \n",
        "\n",
        "                                                url  \n",
        "0  http://github.com/drupalprojects/views_slideshow  \n",
        "1                 http://github.com/asael2/satenspr  \n",
        "2       http://github.com/sepulworld/puppet-mongodb  \n",
        "3      http://github.com/rainux/vim-desert-warm-256  \n",
        "4                     http://github.com/ten0s/emacs  \n",
        "\n",
        "[5 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load labelled data\n",
      "\n",
      "train_csv = 'data/{}_repos_training.csv'.format(domain)\n",
      "sm = pd.read_csv(train_csv, header = 0, sep='\\t')\n",
      "# construct term list by splitting repo names, cleaning out punctuation, separating Capitalised words\n",
      "sm_terms = [re.sub('[\\d]{2,}|[_\\W*]', ' ', s.split('/')[1]) for s in sm['repo']]\n",
      "sm_terms = [' '.join(re.findall('[A-Z][a-z]*', t)).lower() if re.match('[A-Z]', t) else t.lower() for t in sm_terms]\n",
      "# sm_terms = [re.sub('[\\d*_\\W*]', ' ', s.split('/')[1]).lower() for s in sm.repo]\n",
      "sm_words = [w for s in sm_terms for w in s.split(' ') if len(w) >= 2]\n",
      "stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)\n",
      "sm_words_stemmed = [stemmer.stem(w) for w in sm_words]\n",
      "\n",
      "#choose the top words\n",
      "term_count  = 200\n",
      "sm_words_counts = collections.Counter(sm_words_stemmed)\n",
      "terms = sm_words_counts.most_common(term_count)\n",
      "keywords = [t[0] for t in terms]\n",
      "\n",
      "# construct an indicator matrix for the key terms\n",
      "indicator_m = np.zeros([sm.shape[0], len(keywords)])\n",
      "for i in range(0, len(sm_terms)):\n",
      "        for j in sm_terms[i].split(' '):\n",
      "                word = stemmer.stem(j)\n",
      "                if word in keywords:\n",
      "                    indicator_m[i,keywords.index(word)] = 1.0\n",
      "            \n",
      "response_domain = 'is_{}'.format(domain)\n",
      "# sm[response_domain]  = sm[response_domain].str.lower()\n",
      "# y = sm[response_domain] == 'true'\n",
      "y = sm[response_domain]\n",
      "\n",
      "# create 50% split between training and test data\n",
      "split = np.random.randint(0, sm.shape[0], sm.shape[0]/2)\n",
      "domain_training  = pd.DataFrame(indicator_m[split,], columns =  keywords)\n",
      "#response variable\n",
      "domain_training[response_domain] = y[split].values\n",
      "\n",
      "#create test data by masking training data\n",
      "mask = np.ones(len(y), dtype=bool)\n",
      "mask[split] = False\n",
      "domain_test = pd.DataFrame(indicator_m[mask], columns = keywords)\n",
      "domain_test[response_domain] = y[mask].values\n",
      "\n",
      "# training for naive bayes\n",
      "\n",
      "#the number of examples  in total\n",
      "n = domain_training.shape[0]\n",
      "#the number of social media examples\n",
      "n_c = sum(domain_training[response_domain] == True)\n",
      "#the counts of each term in the social media examples\n",
      "n_jc = domain_training.ix[domain_training[response_domain] == True, 0 : term_count].sum(axis=0)\n",
      "#the overall probability of a given example being social media\n",
      "theta_c = n_c/float(n)\n",
      "#the overall probability of a given example not being social media\n",
      "theta_0 = (n-n_c)/float(n)\n",
      "alpha = 1.5\n",
      "beta = 1.5\n",
      "#the probability of each term occurring given a social media example\n",
      "theta_jc = (n_jc + alpha -1)/(n_c + alpha + beta -2)\n",
      "#the probability of each term occurring for a non-social media example\n",
      "theta_j0 = (domain_training.ix[domain_training[response_domain] != True, 0 : term_count].sum(axis=0) +alpha-1)/(n - n_c  + alpha + beta - 2)\n",
      "# the weights for each term given social media examples\n",
      "w_jc = np.log((theta_jc*(1-theta_j0))/(theta_j0*(1-theta_jc)))\n",
      "#the weights of the bias term\n",
      "w_0c = np.sum(np.log ((1-theta_jc)/(1-theta_j0))) + np.log(theta_c/theta_0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2. Construct an indicator matrix for all the common terms in the names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = sort([w for s in sc_terms for w in s.split(' ') if len(w) >= 2])\n",
      "\n",
      "# print ([len(w) for w in words])\n",
      "print words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['aaydaaigd3caa' 'accumulo' 'accumulo' ..., 'zip' 'zip' 'zookeeper']\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "c_words = collections.Counter(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "c_words.most_common(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "[('redis', 194),\n",
        " ('d3', 125),\n",
        " ('mysql', 90),\n",
        " ('mongodb', 81),\n",
        " ('elasticsearch', 80),\n",
        " ('titanium', 65),\n",
        " ('compass', 62),\n",
        " ('oauth2', 54),\n",
        " ('node', 53),\n",
        " ('memcached', 51),\n",
        " ('cassandra', 44),\n",
        " ('solr', 43),\n",
        " ('neo4j', 41),\n",
        " ('android', 41),\n",
        " ('indicator', 38),\n",
        " ('client', 37),\n",
        " ('sphinx', 33),\n",
        " ('php', 32),\n",
        " ('couchdb', 29),\n",
        " ('python', 29),\n",
        " ('my', 28),\n",
        " ('pager', 28),\n",
        " ('hive', 28),\n",
        " ('view', 27),\n",
        " ('riak', 27),\n",
        " ('puppet', 26),\n",
        " ('oracle', 25),\n",
        " ('sqlite', 24),\n",
        " ('hbase', 24),\n",
        " ('archive', 24),\n",
        " ('postgresql', 23),\n",
        " ('jade', 23),\n",
        " ('plugin', 23),\n",
        " ('test', 20),\n",
        " ('example', 18),\n",
        " ('ingress', 18),\n",
        " ('auth', 18),\n",
        " ('net', 17),\n",
        " ('java', 17),\n",
        " ('mongo', 16),\n",
        " ('data', 15),\n",
        " ('spring', 15),\n",
        " ('modules', 14),\n",
        " ('lite', 14),\n",
        " ('titan', 14),\n",
        " ('search', 14),\n",
        " ('couchbase', 13),\n",
        " ('server', 12),\n",
        " ('mobile', 12),\n",
        " ('river', 12)]"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)\n",
      "words_stemmed = [stemmer.stem(w) for w in words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "c  = collections.Counter(words_stemmed)\n",
      "c.most_common(50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "[(u'redi', 194),\n",
        " ('d3', 125),\n",
        " (u'mysql', 90),\n",
        " (u'mongodb', 81),\n",
        " (u'elasticsearch', 80),\n",
        " (u'titanium', 65),\n",
        " (u'compass', 63),\n",
        " (u'oauth2', 54),\n",
        " (u'node', 53),\n",
        " (u'memcach', 51),\n",
        " (u'cassandra', 44),\n",
        " (u'solr', 43),\n",
        " (u'neo4j', 41),\n",
        " (u'android', 41),\n",
        " (u'indic', 40),\n",
        " (u'client', 37),\n",
        " (u'archiv', 37),\n",
        " (u'sphinx', 33),\n",
        " (u'php', 32),\n",
        " (u'plugin', 30),\n",
        " (u'python', 29),\n",
        " (u'couchdb', 29),\n",
        " (u'hive', 28),\n",
        " ('my', 28),\n",
        " (u'pager', 28),\n",
        " (u'view', 27),\n",
        " (u'riak', 27),\n",
        " (u'puppet', 26),\n",
        " (u'oracl', 25),\n",
        " (u'sqlite', 24),\n",
        " (u'hbase', 24),\n",
        " (u'titan', 24),\n",
        " (u'modul', 23),\n",
        " (u'postgresql', 23),\n",
        " (u'jade', 23),\n",
        " (u'exampl', 23),\n",
        " (u'test', 21),\n",
        " (u'auth', 18),\n",
        " (u'ingress', 18),\n",
        " (u'net', 17),\n",
        " (u'java', 17),\n",
        " (u'mongo', 16),\n",
        " (u'data', 15),\n",
        " (u'spring', 15),\n",
        " (u'search', 14),\n",
        " (u'lite', 14),\n",
        " (u'mobil', 13),\n",
        " (u'couchbas', 13),\n",
        " (u'sampl', 13),\n",
        " (u'river', 12)]"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}