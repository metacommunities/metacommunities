# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# # Validate topics of repos
# 
# The repository topics are generated by text matching. But that creates a huge amount of noise in the classifications.
# Is it possible to build classifiers that use some training data for each topic to clean that up a bit?
# 
# Here I use a Bernoulli Naive Bayes to clean up a given domains. It does mean providing a training set for each domain. 

# <codecell>

import redis
import pandas as pd
import re
import nltk.stem
import re
import collections
import nltk
import numpy as np
from sklearn.metrics import confusion_matrix
import sklearn.naive_bayes as nb

# <markdowncell>

# ## Treat the names as source of features to be extracted by cleaning and splitting, etc

# <codecell>

red = redis.Redis(db='1')
keys = red.keys('*')

# <codecell>

repo_keys = [k  for k in keys if k.find(':') == -1]
repo_keys

# <markdowncell>

# 1. Create a training set for domain repos

# <codecell>

domain = 'tests'

# <codecell>

def create_domain_training_set(domain):
    sc = red.srandmember(domain, 2000)
    # sc_terms = [re.sub('[\d*_\W*]', ' ', s.split('/')[1]).lower() for s in sc]
    sc_terms = [re.sub('[\d]{2,}|[_\W*]', ' ', s.split('/')[1]) for s in sc]
    sc_terms = [' '.join(re.findall('[A-Z][a-z]*', t)).lower() if re.match('[A-Z]', t) else t.lower() for t in sc_terms]
    url = ['http://github.com/'+s for s in sc]
    training = pd.DataFrame({'repo':sc, 'terms':sc_terms, 'url':url, 'is_'+domain:True})
    training.to_csv('data/'+domain+'_repos.csv', index = None)
    training.to_csv('data/'+domain+'_repos_training.csv',  index = None)

# <codecell>

create_domain_training_set(domain)

# <codecell>

# after categorising 2000 repos by hand!
# note that it the csv will now be call x_training.csv
training_df = pd.read_csv('data/{0}_repos_training.csv'.format(domain), header=0, sep='\t')
# training_df['is_{0}'.format(domain)] = training_df['is_{0}'.format(domain)].str.lower()
training_df.head()

# <codecell>

# load labelled data

train_csv = 'data/{}_repos_training.csv'.format(domain)
sm = pd.read_csv(train_csv, header = 0, sep='\t')
# construct term list by splitting repo names, cleaning out punctuation, separating Capitalised words
sm_terms = [re.sub('[\d]{2,}|[_\W*]', ' ', s.split('/')[1]) for s in sm['repo']]
sm_terms = [' '.join(re.findall('[A-Z][a-z]*', t)).lower() if re.match('[A-Z]', t) else t.lower() for t in sm_terms]
# sm_terms = [re.sub('[\d*_\W*]', ' ', s.split('/')[1]).lower() for s in sm.repo]
sm_words = [w for s in sm_terms for w in s.split(' ') if len(w) >= 2]
stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)
sm_words_stemmed = [stemmer.stem(w) for w in sm_words]

#choose the top words
term_count  = 200
sm_words_counts = collections.Counter(sm_words_stemmed)
terms = sm_words_counts.most_common(term_count)
keywords = [t[0] for t in terms]

# construct an indicator matrix for the key terms
indicator_m = np.zeros([sm.shape[0], len(keywords)])
for i in range(0, len(sm_terms)):
        for j in sm_terms[i].split(' '):
                word = stemmer.stem(j)
                if word in keywords:
                    indicator_m[i,keywords.index(word)] = 1.0
            
response_domain = 'is_{}'.format(domain)
# sm[response_domain]  = sm[response_domain].str.lower()
# y = sm[response_domain] == 'true'
y = sm[response_domain]

# create 50% split between training and test data
split = np.random.randint(0, sm.shape[0], sm.shape[0]/2)
domain_training  = pd.DataFrame(indicator_m[split,], columns =  keywords)
#response variable
domain_training[response_domain] = y[split].values

#create test data by masking training data
mask = np.ones(len(y), dtype=bool)
mask[split] = False
domain_test = pd.DataFrame(indicator_m[mask], columns = keywords)
domain_test[response_domain] = y[mask].values

# training for naive bayes

#the number of examples  in total
n = domain_training.shape[0]
#the number of social media examples
n_c = sum(domain_training[response_domain] == True)
#the counts of each term in the social media examples
n_jc = domain_training.ix[domain_training[response_domain] == True, 0 : term_count].sum(axis=0)
#the overall probability of a given example being social media
theta_c = n_c/float(n)
#the overall probability of a given example not being social media
theta_0 = (n-n_c)/float(n)
alpha = 1.5
beta = 1.5
#the probability of each term occurring given a social media example
theta_jc = (n_jc + alpha -1)/(n_c + alpha + beta -2)
#the probability of each term occurring for a non-social media example
theta_j0 = (domain_training.ix[domain_training[response_domain] != True, 0 : term_count].sum(axis=0) +alpha-1)/(n - n_c  + alpha + beta - 2)
# the weights for each term given social media examples
w_jc = np.log((theta_jc*(1-theta_j0))/(theta_j0*(1-theta_jc)))
#the weights of the bias term
w_0c = np.sum(np.log ((1-theta_jc)/(1-theta_j0))) + np.log(theta_c/theta_0)

# <markdowncell>

# 2. Construct an indicator matrix for all the common terms in the names

# <codecell>

words = sort([w for s in sc_terms for w in s.split(' ') if len(w) >= 2])

# print ([len(w) for w in words])
print words

# <codecell>

c_words = collections.Counter(words)

# <codecell>

c_words.most_common(50)

# <codecell>

stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)
words_stemmed = [stemmer.stem(w) for w in words]

# <codecell>

c  = collections.Counter(words_stemmed)
c.most_common(50)

# <codecell>


