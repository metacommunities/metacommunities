#!/bin/bash

# export tables from sqlite database to CSV
mkdir 'stackoverflow.com_CSV'

## export the whole table to a CSV
#function dump {
#sqlite3 'stack-overflow-dump.sqlite3' <<!
#.headers on
#.mode csv
#.output stackoverflow.com_CSV/$1.csv
#SELECT * FROM $1;
#!
#}

#dump 'users'
#dump 'badges'
#dump 'votes'


## some tables are too big, the following will cut up a table (by rows) and
## export each chunk to CSV
#function dumplimit {
#  # row count
#  echo "counting ${1}"
#  SQLEXPR="SELECT COUNT(*) FROM ${1};"
#  COUNT=`sqlite3 -line stack-overflow-dump.sqlite3 "${SQLEXPR}"`
#  COUNT=`echo $COUNT | cut -c 12-`

#  # how rows for one CSV
#  PROP=$(expr $COUNT / $2)
#  
#  # export
#  echo -n "exporting ${1} "
#  for i in $(seq 1 $PROP $COUNT)
#    do
#    j=$(expr $i + $PROP - 1)

#sqlite3 'stack-overflow-dump.sqlite3' <<!
#.headers on
#.mode csv
#.output stackoverflow.com_CSV/$1_$j.csv
#SELECT * FROM $1 LIMIT $i, $j;
#!

#    echo -n '. '
#  done
#  echo 'done'
#}

## comments table needs splitting in half
#dumplimit 'comments' 2


# 'posts' is very special. it not only needs splitting up because it's too big,
# but we also need to seperate the post types; 1 (questions) and 2 (answers).
function postdumplimit {
  # row count
  echo "counting posts (type ${1}) "
  SQLEXPR="SELECT COUNT(*) FROM posts WHERE PostTypeId = ${1};"
  COUNT=`sqlite3 -line stack-overflow-dump.sqlite3 "${SQLEXPR}"`
  COUNT=`echo $COUNT | cut -c 12-`
  
  # how rows for one CSV
  PROP=$(expr $COUNT / $2)
  
  # export
  echo -n "exporting posts (type ${1}) "
  for i in $(seq 1 $PROP $COUNT)
    do
    j=$(expr $i + $PROP - 1)
    SQLEXPR="SELECT * FROM posts WHERE PostTypeId = ${1} LIMIT ${i}, ${j};"
    
sqlite3 'stack-overflow-dump.sqlite3' <<!
.headers on
.mode csv
.output stackoverflow.com_CSV/posts_type$1_$j.csv
$SQLEXPR
!

    echo -n '. '
  done
  echo 'done'
}

# questions
postdumplimit 1 10
# answers
postdumplimit 2 10 
