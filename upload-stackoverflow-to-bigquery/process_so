#!/bin/bash

# extract just Stack Overflow
7za e 'Content/stackoverflow.com.7z.001' -o'stackoverflow.com'

# create sqlite database from the extracted XML files
python create_sqlite_db.py -i'stackoverflow.com'

# export tables from sqlite database to CSV
./export_so

# compress each CSV for uploading
mkdir 'stackoverflow.com_ZIP'

FILES=`ls stackoverflow.com_CSV`
for f in $FILES
  do
  gzip -cv stackoverflow.com_CSV/$f > stackoverflow.com_ZIP/$f.gz
done

# upload stackoverflow to bigquery
# make sure you've ran 'bq init' to setup your credentials
QST=`ls stackoverflow.com_ZIP/posts_type1*`
for q in $QST
  do
  bq load --source_format=CSV <destination_table> <data_source_uri> <table_schema>
done

ANS=`ls stackoverflow.com_ZIP/posts_type2*`
for a in $ANS
  do
  bq load --source_format=CSV <destination_table> <data_source_uri> <table_schema>
done

