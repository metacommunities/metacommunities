#!/bin/bash

SODIR='stackoverflow.com'
JSONDIR="${SODIR}_JSON"

# extract just Stack Overflow
7za e 'Content/stackoverflow.com.7z.001' -o$SODIR

# create sqlite database from the extracted XML files
python create_sqlite_db.py -i$SODIR


# ==============================================================================
# convert the XML in to JSON
# ==============================================================================
python convert_xml_to_json.py -i$SODIR -t'Users'
python convert_xml_to_json.py -i$SODIR -t'Badges'
python convert_xml_to_json.py -i$SODIR -t'Votes'
python convert_xml_to_json.py -i$SODIR -t'Comments'
python convert_xml_to_json.py -i$SODIR -t'Posts'


# ==============================================================================
# compress each JSON for storage (and uploading, if they are less than 1GB)
# ==============================================================================
FILES=`ls $JSONDIR/*json`
for f in $FILES
  do
  gzip -cv $JSONDIR/$f > $JSONDIR/$f.gz
done


# ==============================================================================
# upload stackoverflow to bigquery
# ==============================================================================
# make sure you've ran 'bq init' to setup your credentials
PROJECT='stack_overflow'

bq mk $PROJECT

# Users
bq load --source_format=NEWLINE_DELIMITED_JSON "${PROJECT}.Users" \
  "${JSONDIR}/Users.json.gz" 'schema/Users'

# Badges
bq load --source_format=NEWLINE_DELIMITED_JSON "${PROJECT}.Badges" \
  "${JSONDIR}/Badges.json.gz" 'schema/Badges'

# Votes
bq load --source_format=NEWLINE_DELIMITED_JSON "${PROJECT}.Votes" \
  "${JSONDIR}/Votes.json.gz" 'schema/Votes'

# Comments
bq load --source_format=NEWLINE_DELIMITED_JSON "${PROJECT}.Comments" \
  "${JSONDIR}/Comments.json" 'schema/Comments'

# Posts
bq load --source_format=NEWLINE_DELIMITED_JSON "${PROJECT}.Posts" \
  "${JSONDIR}/Posts.json" 'schema/Posts'

